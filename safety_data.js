// --- START OF FILE data.js ---

// --- Central Data Source ---
// This array holds the raw data for the AI Safety ecosystem entries.
// The main script in index.html will process this array (e.g., adding icons).
const rawAiSafetyData = [
  // AI Labs
  { id: "openai", name: "OpenAI", url: "https://openai.com/", category: "AI Lab", importance: 5, description: "San Francisco-based capabilities lab, creator of ChatGPT. Historically significant safety team, though experienced departures in 2024.", logoUrl: "https://openai.com/icon.svg?d5c238c2cf2e4f08", subCategoryIcon: "bi-building", subCategoryText: "AI Lab (Capabilities & Safety Research)", safetyLinks: [{ text: "Safety Overview", url: "https://openai.com/safety", icon: "bi-shield-check", title:"OpenAI's official safety policies" }, { text: "Research", url: "https://openai.com/research", icon: "bi-journal-richtext", title:"OpenAI's research publications" }] },
  { id: "deepmind", name: "Google DeepMind", url: "https://www.deepmind.com/", category: "AI Lab", importance: 5, description: "Major London-based AI capabilities lab (Google-owned) with a strong, long-standing safety research team. Created AlphaGo, AlphaFold, Gemini.", logoUrl: "https://www.gstatic.com/images/branding/productlogos/google_deepmind/v3/web-96dp/logo_google_deepmind_color_2x_web_96dp.png", subCategoryIcon: "bi-building", subCategoryText: "AI Lab (Capabilities & Safety Research)", safetyLinks: [{ text: "Responsibility & Safety", url: "https://deepmind.google/discover/responsibility-safety/", icon: "bi-shield-check", title:"DeepMind's approach to responsibility" }, { text: "Blog (incl. Safety)", url: "https://deepmind.google/discover/blog/", icon: "bi-journal-text", title:"DeepMind's blog for updates" }] },
  { id: "anthropic", name: "Anthropic", url: "https://www.anthropic.com/", category: "AI Lab", importance: 5, description: "Research lab focusing heavily on LLM alignment and safety, particularly interpretability. Created Claude. Known for its safety-first mission.", logoUrl: "https://static.cdnlogo.com/logos/a/68/anthropic.svg", subCategoryIcon: "bi-shield-shaded", subCategoryText: "AI Lab (Safety-Focused)", safetyLinks: [{ text: "Safety Research", url: "https://www.anthropic.com/research", icon: "bi-journal-richtext", title:"Anthropic's safety research papers" }, { text: "Constitutional AI", url: "https://www.anthropic.com/constitutional-ai", icon: "bi-rulers", title:"Anthropic's Constitutional AI method" }] },
  { id: "xai", name: "xAI", url: "https://x.ai/", category: "AI Lab", importance: 3, description: "Capabilities lab led by Elon Musk aiming to \"understand the universe.\" Created Grok. Safety stance less defined than others.", logoUrl: "https://logowik.com/content/uploads/images/xai905.logowik.com.webp", subCategoryIcon: "bi-building", subCategoryText: "AI Lab (Capabilities Research)", safetyLinks: [{ text: "About/Mission", url: "https://x.ai/about", icon: "bi-info-circle", title:"Learn about xAI's mission" }] },
  { id: "ssi", name: "Safe Superintelligence Inc. (SSI)", url: "https://ssi.inc/", category: "AI Lab", importance: 3, description: "Research lab founded by Ilya Sutskever focused explicitly on building safe superintelligence. Newer, impact TBD.", logoUrl: null, subCategoryIcon: "bi-shield-lock-fill", subCategoryText: "AI Lab (Safety-Focused)", safetyLinks: [{ text: "Mission", url: "https://ssi.inc/", icon: "bi-bullseye", title:"SSI's mission statement" }] },
  { id: "deepseek", name: "DeepSeek", url: "https://www.deepseek.com/", category: "AI Lab", importance: 2, description: "Chinese capabilities lab developing and releasing open-weights LLMs (e.g., DeepSeek-R1). Important international player.", logoUrl: "https://logowik.com/content/uploads/images/deepseek-ai4760.logowik.com.webp", subCategoryIcon: "bi-building", subCategoryText: "AI Lab (Capabilities Research, China)", safetyLinks: [{ text: "GitHub/Models", url: "https://github.com/deepseek-ai", icon: "bi-github", title:"DeepSeek's models on GitHub" }] },
  { id: "obelisk", name: "Obelisk (at Astera)", url: "https://astera.org/agi-program/", category: "AI Lab", importance: 2, description: "Research team aiming to engineer AGI using an exploratory approach inspired by cognitive science/neuroscience.", logoUrl: "https://static.cdnlogo.com/logos/a/29/astera.svg", subCategoryIcon: "bi-clipboard-data", subCategoryText: "Research Team (AGI Engineering)", safetyLinks: [{ text: "AGI Program Details", url: "https://astera.org/agi-program/", icon: "bi-journal-richtext", title:"Astera's Obelisk AGI program details" }] },

  // Academic/Research
  { id: "arc", name: "Alignment Research Center (ARC)", url: "https://www.alignment.org/theory/", category: "Academic/Research", importance: 4, description: "Influential research organization trying to formalize mechanistic explanations of neural network behavior (e.g., ELK).", subCategoryIcon: "bi-diagram-3", subCategoryText: "Research Center (Conceptual)" },
  { id: "chai", name: "Center for Human-Compatible AI (CHAI)", url: "https://humancompatible.ai/", category: "Academic/Research", importance: 4, description: "UC Berkeley group led by Stuart Russell, developing theory (CIRL) and techniques for provably beneficial AI.", subCategoryIcon: "bi-bank", subCategoryText: "University Center (Conceptual/Technical)" },
  { id: "cais", name: "Center for AI Safety (CAIS)", url: "https://safe.ai/", category: "Academic/Research", importance: 4, description: "San Francisco nonprofit conducting safety research, field building, and advocating for safety standards.", subCategoryIcon: "bi-search-heart", subCategoryText: "Research & Advocacy Nonprofit" },
  { id: "redwood", name: "Redwood Research", url: "https://www.redwoodresearch.org/", category: "Academic/Research", importance: 4, description: "Nonprofit researching interpretability and alignment, also consults governments and AI labs on safety practices.", subCategoryIcon: "bi-lightbulb", subCategoryText: "Research Nonprofit (Empirical)" },
  { id: "eleutherai", name: "EleutherAI", url: "https://www.eleuther.ai/", category: "Academic/Research", importance: 3, description: "Open-source research lab focused on interpretability and alignment, operating primarily via Discord.", subCategoryIcon: "bi-code-slash", subCategoryText: "Open Source Research Lab" },
  { id: "metr", name: "Model Evaluation & Threat Research (METR)", url: "https://metr.org/", category: "Academic/Research", importance: 3, description: "Researches, develops, and runs cutting-edge tests of AI capabilities and risks (formerly ARC Evals).", subCategoryIcon: "bi-clipboard-check", subCategoryText: "Evaluation Research Org" },
  { id: "apollo", name: "Apollo Research", url: "https://www.apolloresearch.ai/", category: "Academic/Research", importance: 3, description: "Aims to detect deception via model evaluations and interpretability research; provides policy guidance.", subCategoryIcon: "bi-clipboard-data", subCategoryText: "Research & Policy Advice" },
  { id: "conjecture", name: "Conjecture", url: "https://www.conjecture.dev/", category: "Academic/Research", importance: 3, description: "Alignment startup using a “Cognitive Emulation” approach for controllable LLMs and safety challenges.", subCategoryIcon: "bi-shield-check", subCategoryText: "AI Startup (Safety Focused)" },
  { id: "farai", name: "Frontier Alignment Research (FAR.AI)", url: "https://far.ai/", category: "Academic/Research", importance: 3, description: "Incubates and accelerates research agendas for trustworthy and beneficial AI.", subCategoryIcon: "bi-box-seam", subCategoryText: "Research Incubator" },
  { id: "ought", name: "Ought", url: "https://ought.org/", category: "Academic/Research", importance: 3, description: "Product-driven lab developing mechanisms for delegating reasoning to ML systems (built Elicit).", subCategoryIcon: "bi-wrench-adjustable-circle", subCategoryText: "Product/Research Lab" },
  { id: "acs", name: "Alignment of Complex Systems Research Group (ACS)", url: "https://acsresearch.org/", category: "Academic/Research", importance: 2, description: "Studies multi-agent systems (humans and AI). Based at Charles University, Prague.", subCategoryIcon: "bi-bank", subCategoryText: "University Research Group" },
  { id: "atlas", name: "Atlas Computing", url: "https://atlascomputing.org/", category: "Academic/Research", importance: 2, description: "Prototyping AI tools for formal specification generation to improve code verification.", subCategoryIcon: "bi-gear-wide-connected", subCategoryText: "R&D Nonprofit" },
  { id: "alignedai", name: "Aligned AI", url: "https://buildaligned.ai/", category: "Academic/Research", importance: 2, description: "Oxford-based startup using mathematical techniques for safe off-distribution generalization.", subCategoryIcon: "bi-shield-check", subCategoryText: "AI Startup (Safety Focused)" },
  { id: "cyborgism", name: "Cyborgism (Concept)", url: "https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism", category: "Academic/Research", importance: 2, description: "Strategy exploring human-in-the-loop systems to accelerate alignment research.", subCategoryIcon: "bi-signpost-split", subCategoryText: "Research Strategy" },
  { id: "yampolskiy", name: "Dr. Roman V. Yampolskiy", url: "http://cecs.louisville.edu/ry/", category: "Academic/Research", importance: 2, description: "Professor researching AI safety, cybersecurity background, numerous publications.", subCategoryIcon: "bi-person", subCategoryText: "Academic Researcher" },
  { id: "hadfieldmenell", name: "Dylan Hadfield-Menell", url: "https://people.csail.mit.edu/dhm/", category: "Academic/Research", importance: 2, description: "Assistant professor at MIT working on agent alignment. Runs the Algorithmic Alignment Group.", subCategoryIcon: "bi-person", subCategoryText: "Academic Researcher (MIT)" },
  { id: "grayswan", name: "Gray Swan", url: "https://www.grayswan.ai/", category: "Academic/Research", importance: 2, description: "For-profit developing tools to assess AI model risks and building its own safety-focused models.", subCategoryIcon: "bi-shield-check", subCategoryText: "AI Startup (Safety Tools)" },
  { id: "wentworth", name: "John Wentworth", url: "https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans", category: "Academic/Research", importance: 2, description: "Independent alignment researcher working on selection theorems, abstraction, and agency.", subCategoryIcon: "bi-person", subCategoryText: "Independent Researcher" },
  { id: "kasl", name: "Krueger AI Safety Lab (KASL)", url: "https://www.kasl.ai/publications/", category: "Academic/Research", importance: 2, description: "AI safety research group at the University of Cambridge, led by David Krueger.", subCategoryIcon: "bi-bank", subCategoryText: "University Research Lab (Cambridge)" },
  { id: "mai", name: "Meaning Alignment Institute (MAI)", url: "https://meaningalignment.org/", category: "Academic/Research", importance: 2, description: "Applies expertise in meaning and human values to AI alignment and post-AGI futures.", subCategoryIcon: "bi-palette", subCategoryText: "Research Organization (Human Values)" },
  { id: "mitaag", name: "MIT Algorithmic Alignment Group", url: "https://algorithmicalignment.csail.mit.edu/", category: "Academic/Research", importance: 2, description: "Working towards better conceptual understanding, algorithmic techniques, and policies for safer AI.", subCategoryIcon: "bi-bank", subCategoryText: "University Research Group (MIT)" },
  { id: "modelingcoop", name: "Modeling Cooperation", url: "https://www.modelingcooperation.com/", category: "Academic/Research", importance: 2, description: "Conducting research on improving cooperation in competition for transformative AI development.", subCategoryIcon: "bi-diagram-3", subCategoryText: "Research Focus (Cooperation)" },
  { id: "nyuarg", name: "NYU Alignment Research Group (ARG)", url: "https://wp.nyu.edu/arg/", category: "Academic/Research", importance: 2, description: "Conducts empirical work with language models aiming to address longer-term AI impacts.", subCategoryIcon: "bi-bank", subCategoryText: "University Research Group (NYU)" },
  { id: "orthogonal", name: "Orthogonal", url: "https://orxl.org", category: "Academic/Research", importance: 2, description: "Formal alignment organization focused on agent foundations. Has a public Discord.", subCategoryIcon: "bi-bounding-box-circles", subCategoryText: "Research Organization (Agent Foundations)" },
  { id: "softmax", name: "Softmax", url: "https://www.softmax.com/", category: "Academic/Research", importance: 2, description: "Develops a theory of \"organic alignment\" for human-digital agent cooperation.", subCategoryIcon: "bi-recycle", subCategoryText: "Research Organization (Organic Alignment)" },
  { id: "byrnes", name: "Steve Byrnes's Brain-Like AGI Safety", url: "https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8", category: "Academic/Research", importance: 2, description: "Brain-inspired framework using neuroscience/RL insights for aligned AGI design.", subCategoryIcon: "bi-brain", subCategoryText: "Research Framework" },
  { id: "shardtheory", name: "Team Shard (Shard Theory)", url: "https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview", category: "Academic/Research", importance: 2, description: "Small team trying to find reward functions which reliably instill values (Shard Theory).", subCategoryIcon: "bi-gem", subCategoryText: "Research Team/Theory" },
  { id: "timaeus", name: "Timaeus", url: "https://timaeus.co/", category: "Academic/Research", importance: 2, description: "Uses singular learning theory to understand how training data determines model behavior.", subCategoryIcon: "bi-infinity", subCategoryText: "Research Nonprofit (Singular Learning Theory)" },
  { id: "transluce", name: "Transluce", url: "https://transluce.org/", category: "Academic/Research", importance: 2, description: "Builds open source, scalable tools to understand, analyze, and steer AI systems.", subCategoryIcon: "bi-code-square", subCategoryText: "Research Lab (Open Source Tools)" },
  { id: "camcbl", name: "University of Cambridge Comp. & Bio. Learning Lab (CBL)", url: "https://cbl.eng.cam.ac.uk/", category: "Academic/Research", importance: 2, description: "Uses engineering approaches to understand the brain and develop AI learning systems.", subCategoryIcon: "bi-bank", subCategoryText: "University Research Lab (Cambridge)" },
  { id: "aoi", name: "AI Objectives Institute (AOI)", url: "https://ai.objectives.institute/", category: "Academic/Research", importance: 2, description: "Builds AI tools to defend/enhance human agency via novel AI capabilities research.", subCategoryIcon: "bi-person-check", subCategoryText: "Research Nonprofit (Human Agency)" },
  { id: "equistamp", name: "EquiStamp", url: "https://www.equistamp.com/", category: "Academic/Research", importance: 1, description: "Platform for evaluating AI model capabilities to assess trustworthiness.", subCategoryIcon: "bi-pc-display-horizontal", subCategoryText: "Platform/Startup" },
  { id: "cavendish", name: "Cavendish Labs", url: "https://cavendishlabs.org/", category: "Academic/Research", importance: 1, description: "AI safety (and pandemic prevention) research community based in Vermont, USA.", subCategoryIcon: "bi-people-fill", subCategoryText: "Research Community" },

   // Policy/Gov
   { id: "govai", name: "Centre for the Governance of AI (GovAI)", url: "https://www.governance.ai/", category: "Policy/Gov", importance: 4, description: "Highly influential AI governance research group, producing policy research and running career programs.", subCategoryIcon: "bi-bank", subCategoryText: "University Research Group (Oxford)" },
   { id: "cset", name: "Center for Security and Emerging Technology (CSET)", url: "https://cset.georgetown.edu/", category: "Policy/Gov", importance: 4, description: "Georgetown University think tank providing data-driven analysis on security implications of emerging tech.", subCategoryIcon: "bi-building-columns", subCategoryText: "University Think Tank (Georgetown)" },
   { id: "ukaisi", name: "UK AI Safety Institute (UK AISI)", url: "https://www.aisi.gov.uk/", category: "Policy/Gov", importance: 4, description: "UK government organisation researching, testing safety, measuring impacts, and shaping global policy.", subCategoryIcon: "bi-flag-fill", subCategoryText: "Government Institute (UK)" },
   { id: "usaisi", name: "U.S. AI Safety Institute (USAISI)", url: "https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute", category: "Policy/Gov", importance: 4, description: "US government organization (NIST) advancing AI safety science, practice, and adoption.", subCategoryIcon: "bi-flag-fill", subCategoryText: "Government Institute (US)" },
   { id: "fli", name: "Future of Life Institute (FLI)", url: "https://futureoflife.org/", category: "Policy/Gov", importance: 4, description: "Works on steering transformative tech via outreach, policy advocacy, grantmaking, and event organisation.", subCategoryIcon: "bi-megaphone", subCategoryText: "Advocacy, Policy, Funding" }, // Primarily policy/advocacy focus here
   { id: "miri", name: "Machine Intelligence Research Institute (MIRI)", url: "https://intelligence.org/", category: "Policy/Gov", importance: 4, description: "Original AI safety research org (Yudkowsky), now more focused on policy and public outreach. Foundational concepts.", subCategoryIcon: "bi-search-heart", subCategoryText: "Research, Policy, Advocacy" }, // Primarily policy/advocacy focus here
   { id: "clr", name: "Center on Long-Term Risk (CLR)", url: "https://longtermrisk.org/", category: "Policy/Gov", importance: 3, description: "Focuses on AI safety research, grants, and community, particularly around conflict scenarios and cooperation.", subCategoryIcon: "bi-shield-exclamation", subCategoryText: "Research, Funding, Community (X-Risk Focus)" },
   { id: "pai", name: "Partnership on AI (PAI)", url: "https://partnershiponai.org/", category: "Policy/Gov", importance: 3, description: "Convenes academic, civil society, industry, and media organizations to create solutions for beneficial AI.", subCategoryIcon: "bi-people-fill", subCategoryText: "Multi-stakeholder Org" },
   { id: "cser", name: "Centre for the Study of Existential Risk (CSER)", url: "https://www.cser.ac.uk/", category: "Policy/Gov", importance: 3, description: "Cambridge interdisciplinary centre dedicated to the study and mitigation of existential risks, including AI.", subCategoryIcon: "bi-bank", subCategoryText: "University Center (Cambridge)" },
   { id: "epochai", name: "Epoch AI", url: "https://epochai.org/", category: "Policy/Gov", importance: 3, description: "Research institute investigating key trends and questions shaping the trajectory and governance of AI.", subCategoryIcon: "bi-graph-up", subCategoryText: "Research Institute (Trends & Governance)" }, // Could also be Forecasting
   { id: "caip", name: "Center for AI Policy (CAIP)", url: "http://aipolicy.us/", category: "Policy/Gov", importance: 3, description: "Nonpartisan research organization developing policy and conducting advocacy to mitigate catastrophic AI risks.", subCategoryIcon: "bi-megaphone-fill", subCategoryText: "Policy & Advocacy Org" },
   { id: "iaps", name: "Institute for AI Policy and Strategy (IAPS)", url: "https://www.iaps.ai/", category: "Policy/Gov", importance: 3, description: "Focuses on policy/standards, compute governance, international governance/China.", subCategoryIcon: "bi-collection", subCategoryText: "Policy Research & Field Building" },
   { id: "rethink", name: "Rethink Priorities", url: "https://rethinkpriorities.org/", category: "Policy/Gov", importance: 3, description: "Researches solutions and strategies to safeguard a flourishing present and future.", subCategoryIcon: "bi-compass", subCategoryText: "Research Nonprofit (Strategy)" }, // Strategy focus
   { id: "aestudio", name: "AE Studio", url: "https://ae.studio/ai-alignment", category: "Policy/Gov", importance: 2, description: "Takes a 'Neglected Approaches' view on alignment, tackling multiple angles (technical & policy).", subCategoryIcon: "bi-person-video3", subCategoryText: "Consultancy/Research" },
   { id: "aigscanada", name: "AI Governance & Safety Canada (AIGS Canada)", url: "https://aigs.ca/", category: "Policy/Gov", importance: 2, description: "Nonpartisan nonprofit working to ensure safe and beneficial AI in Canada.", subCategoryIcon: "bi-flag", subCategoryText: "National Nonprofit (Canada)" },
   { id: "aipi", name: "AI Policy Institute (AIPI)", url: "https://theaipi.org/", category: "Policy/Gov", importance: 2, description: "Engages policymakers/media/public for responsible AI regulation.", subCategoryIcon: "bi-building-columns", subCategoryText: "Policy Institute" },
   { id: "alter", name: "Association for Long Term Existence and Resilience (ALTER)", url: "https://alter.org.il/", category: "Policy/Gov", importance: 2, description: "Israeli nonprofit working to safeguard and improve humanity's future.", subCategoryIcon: "bi-flag", subCategoryText: "Research & Advocacy (Israel)" },
   { id: "beijingaisi", name: "Beijing Institute of AI Safety and Governance (Beijing-AISI)", url: "https://beijing.ai-safety-and-governance.institute/", category: "Policy/Gov", importance: 2, description: "Develops AI safety/governance frameworks in China.", subCategoryIcon: "bi-gear-wide-connected", subCategoryText: "R&D Institute (China)" },
   { id: "carma", name: "Center for AI Risk Management & Alignment (CARMA)", url: "https://carma.org/", category: "Policy/Gov", importance: 2, description: "Interdisciplinary research supporting global AI risk management, policy, technical work.", subCategoryIcon: "bi-intersect", subCategoryText: "Interdisciplinary Research" },
   { id: "cltc", name: "Center for Long-Term Cybersecurity (CLTC)", url: "https://cltc.berkeley.edu/", category: "Policy/Gov", importance: 2, description: "UC Berkeley center bridging research/policy on emerging cybersecurity challenges relevant to AI.", subCategoryIcon: "bi-bank", subCategoryText: "University Center (Cybersecurity)" },
   { id: "cltr", name: "Center for Long-Term Resilience (CLTR)", url: "https://www.longtermresilience.org/", category: "Policy/Gov", importance: 2, description: "Think tank aiming to transform global resilience to extreme risks via governance/decision-making.", subCategoryIcon: "bi-building-columns", subCategoryText: "Think Tank (Resilience)" },
   { id: "clai", name: "Center for Long-term AI (CLAI)", url: "https://long-term-ai.center/", category: "Policy/Gov", importance: 2, description: "China-based interdisciplinary organization exploring AI's long-term societal/ecological impacts.", subCategoryIcon: "bi-tree", subCategoryText: "Research Org (China, Societal/Ecological Focus)" },
   { id: "cfg", name: "Centre for Future Generations (CFG)", url: "https://cfg.eu/", category: "Policy/Gov", importance: 2, description: "Brussels think tank helping governments anticipate/govern impacts of technological change.", subCategoryIcon: "bi-building-columns", subCategoryText: "Think Tank (EU)" },
   { id: "controlai", name: "ControlAI", url: "https://controlai.com/", category: "Policy/Gov", importance: 2, description: "Nonprofit developing policy and conducting public outreach to keep humanity in control of AI.", subCategoryIcon: "bi-megaphone-fill", subCategoryText: "Policy & Advocacy" },
   { id: "eip", name: "Effective Institutions Project", url: "https://effectiveinstitutionsproject.org/", category: "Policy/Gov", importance: 2, description: "Focuses on improving institutional decision-making on critical global challenges.", subCategoryIcon: "bi-diagram-2", subCategoryText: "Advisory & Research (Institutions)" },
   { id: "euaioffice", name: "European AI Office", url: "https://digital-strategy.ec.europa.eu/en/policies/ai-office", category: "Policy/Gov", importance: 2, description: "EU Commission's center for AI expertise, key in implementing the AI Act.", subCategoryIcon: "bi-flag-fill", subCategoryText: "Government Body (EU)" },
   { id: "forethought", name: "Forethought Research", url: "https://www.forethought.org/", category: "Policy/Gov", importance: 2, description: "Small research nonprofit focused on navigating the transition to superintelligence.", subCategoryIcon: "bi-compass", subCategoryText: "Research Nonprofit (Strategy)" },
   { id: "formation", name: "Formation Research", url: "https://formationresearch.com/", category: "Policy/Gov", importance: 2, description: "Researches fundamental lock-in dynamics and power concentration risks.", subCategoryIcon: "bi-lock", subCategoryText: "Research Nonprofit (Lock-in Risk)" },
   { id: "fairargentina", name: "Frontier AI Research (FAIR)", url: "https://frontierartificialinteligenceresearch.com/", category: "Policy/Gov", importance: 2, description: "Argentine nonprofit working on frontier AI safety as a sociotechnical challenge.", subCategoryIcon: "bi-flag", subCategoryText: "Research Nonprofit (Argentina)" },
   { id: "gcri", name: "Global Catastrophic Risk Institute (GCRI)", url: "https://gcrinstitute.org/", category: "Policy/Gov", importance: 2, description: "Small think tank developing solutions for reducing existential risk.", subCategoryIcon: "bi-building-columns", subCategoryText: "Think Tank (X-Risk)" },
   { id: "gpai", name: "Global Partnership on AI (GPAI)", url: "https://www.oecd.org/en/about/programmes/global-partnership-on-artificial-intelligence.html", category: "Policy/Gov", importance: 2, description: "International initiative working to implement OECD AI principles for safe AI.", subCategoryIcon: "bi-globe2", subCategoryText: "International Initiative" },
   { id: "gpi", name: "Global Priorities Institute (GPI)", url: "https://globalprioritiesinstitute.org/", category: "Policy/Gov", importance: 2, description: "Conducts foundational research to inform effective altruism/longtermist decision-making.", subCategoryIcon: "bi-bank", subCategoryText: "University Center (Oxford, Longtermism)" },
   { id: "lawai", name: "Institute for Law & AI (LawAI)", url: "https://law-ai.org/", category: "Policy/Gov", importance: 2, description: "Think tank researching and advising on the legal challenges posed by AI.", subCategoryIcon: "bi-gavel", subCategoryText: "Legal Think Tank" },
   { id: "iaiga", name: "International AI Governance Alliance (IAIGA)", url: "https://www.iaiga.org/", category: "Policy/Gov", importance: 2, description: "Aims to establish a global body for mitigating AI extinction risks and distributing benefits.", subCategoryIcon: "bi-globe2", subCategoryText: "Advocacy/Nonprofit" },
   { id: "iaseai", name: "International Association for Safe & Ethical AI (IASEAI)", url: "https://www.iaseai.org/", category: "Policy/Gov", importance: 2, description: "Aims to ensure safe/ethical AI via policy, research, community building.", subCategoryIcon: "bi-shield-check", subCategoryText: "Nonprofit/Standards" },
   { id: "lcfi", name: "Leverhulme Centre for the Future of Intelligence (CFI)", url: "http://lcfi.ac.uk/", category: "Policy/Gov", importance: 2, description: "Interdisciplinary research centre exploring the nature, ethics, and impact of AI.", subCategoryIcon: "bi-bank", subCategoryText: "University Center (Cambridge)" },
   { id: "median", name: "Median Group", url: "http://mediangroup.org/", category: "Policy/Gov", importance: 2, description: "Models progress in AI, intelligence enhancement, sociology related to x-risks.", subCategoryIcon: "bi-calculator", subCategoryText: "Research Nonprofit (Modeling)" },
   { id: "narrowpath", name: "Narrow Path (Proposal)", url: "https://www.narrowpath.co/", category: "Policy/Gov", importance: 2, description: "Series of policy proposals from ControlAI for surviving superintelligence.", subCategoryIcon: "bi-signpost-split", subCategoryText: "Policy Proposal" },
   { id: "saferai", name: "SaferAI", url: "http://safer-ai.org/", category: "Policy/Gov", importance: 2, description: "French nonprofit working on policy recommendations, research, risk assessment tools.", subCategoryIcon: "bi-flag", subCategoryText: "National Nonprofit (France)" },
   { id: "simoninst", name: "Simon Institute for Longterm Governance", url: "https://www.simoninstitute.ch/", category: "Policy/Gov", importance: 2, description: "Geneva think tank fostering international cooperation on mitigating catastrophic AI risks.", subCategoryIcon: "bi-building-columns", subCategoryText: "Think Tank (Geneva)" },
   { id: "tfs", name: "The Future Society (TFS)", url: "https://thefuturesociety.org/", category: "Policy/Gov", importance: 2, description: "Aims to define, design, deploy projects addressing institutional barriers in AI governance.", subCategoryIcon: "bi-diagram-2", subCategoryText: "Nonprofit (US/EU, Governance Barriers)" },
   { id: "vistainst", name: "Vista Institute for AI Policy", url: "https://vistainstituteai.org/", category: "Policy/Gov", importance: 2, description: "Promotes informed policymaking via research, knowledge-sharing, skill building.", subCategoryIcon: "bi-building-columns", subCategoryText: "Policy Institute" },
   { id: "ailabwatch", name: "AI Lab Watch", url: "https://ailabwatch.org/", category: "Policy/Gov", importance: 2, description: "Collects actions for labs to avert risks and evaluates labs accordingly.", subCategoryIcon: "bi-binoculars", subCategoryText: "Lab Monitoring" },
   { id: "aiplans", name: "AI-Plans", url: "https://ai-plans.com/", category: "Policy/Gov", importance: 2, description: "Ranked compendium of alignment plans and problems; runs hackathons.", subCategoryIcon: "bi-kanban", subCategoryText: "Strategy Platform" },
   //{ id: "thecompendium", name: "The Compendium", url: "https://www.thecompendium.ai/", category: "Policy/Gov", importance: 2, description: "Living document explaining AGI race/risks for non-technical readers.", subCategoryIcon: "bi-journal-text", subCategoryText: "Living Document" }, // Moved to Info/Media

   // Advocacy
   { id: "pauseai", name: "PauseAI", url: "https://pauseai.info", category: "Advocacy", importance: 3, description: "Campaign group aiming to convince governments to pause AI development via outreach, lobbying, protests.", subCategoryIcon: "bi-sign-stop-fill", subCategoryText: "Campaign Group" },
   { id: "caes", name: "Collective Action for Existential Safety (CAES)", url: "https://existentialsafety.org/", category: "Advocacy", importance: 2, description: "Nonprofit catalyzing collective effort towards reducing x-risk, provides action lists.", subCategoryIcon: "bi-megaphone", subCategoryText: "Nonprofit/Advocacy" },
   { id: "ero", name: "Existential Risk Observatory (ERO)", url: "https://www.existentialriskobservatory.org/", category: "Advocacy", importance: 2, description: "Informing the public debate on existential risks to encourage risk reduction.", subCategoryIcon: "bi-binoculars-fill", subCategoryText: "Public Awareness" },
   { id: "gaim", name: "Global AI Moratorium (GAIM)", url: "https://moratorium.ai", category: "Advocacy", importance: 2, description: "Calling on policymakers to implement a global moratorium on large AI training runs.", subCategoryIcon: "bi-sign-stop-fill", subCategoryText: "Campaign Group" },
   { id: "tmp", name: "The Midas Project (TMP)", url: "https://www.themidasproject.com/", category: "Advocacy", importance: 2, description: "Monitors tech companies, counters propaganda, advocates for responsible AI development.", subCategoryIcon: "bi-eye-fill", subCategoryText: "Watchdog Nonprofit" },
   { id: "aisaf", name: "AI Safety Awareness Foundation (AISAF)", url: "https://aisafetyawarenessfoundation.org/", category: "Advocacy", importance: 1, description: "Volunteer group raising public awareness of AI benefits/risks through workshops.", subCategoryIcon: "bi-person-hearts", subCategoryText: "Volunteer Group" },
   { id: "aigsi", name: "AI Governance and Safety Institute (AIGSI)", url: "https://aigsi.org/", category: "Advocacy", importance: 1, description: "Conducts research/outreach, develops educational materials to improve institutional response.", subCategoryIcon: "bi-broadcast", subCategoryText: "Research & Outreach" },
   { id: "convergence", name: "Convergence Analysis", url: "https://www.convergenceanalysis.org/", category: "Advocacy", importance: 1, description: "Builds reports on AI scenarios/governance and conducts public awareness efforts.", subCategoryIcon: "bi-file-earmark-text", subCategoryText: "Research & Awareness" },
   { id: "cesia", name: "Centre pour la Sécurité de l'IA (CeSIA)", url: "https://www.securite-ia.fr/", category: "Advocacy", importance: 1, description: "French AI safety nonprofit involved in education, advocacy, and research.", subCategoryIcon: "bi-flag", subCategoryText: "National Nonprofit (France)" },

  // Info/Media
  { id: "aisf", name: "AI Safety Fundamentals (AISF)", url: "https://aisafetyfundamentals.com/", category: "Info/Media", importance: 5, description: "Runs the standard introductory courses (Alignment & Governance tracks, 8-11 weeks). Often the first structured step for learners.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Course/Curriculum" }, // Also Field Building
  { id: "lesswrong", name: "LessWrong", url: "https://www.lesswrong.com/", category: "Info/Media", importance: 4, description: "Foundational community blog/forum for rationality and AI safety topics. Essential historical context and ongoing discussion.", subCategoryIcon: "bi-chat-left-text-fill", subCategoryText: "Community Forum/Blog" },
  { id: "alignmentforum", name: "Alignment Forum", url: "https://www.alignmentforum.org/", category: "Info/Media", importance: 4, description: "The primary online forum for technical AI alignment research discussion. Successor to LessWrong for this niche.", subCategoryIcon: "bi-chat-left-text-fill", subCategoryText: "Community Forum/Blog" },
  { id: "eaforum", name: "Effective Altruism Forum", url: "https://forum.effectivealtruism.org/", category: "Info/Media", importance: 3, description: "Main online hub for the Effective Altruism community, often features AI safety discussions/posts.", subCategoryIcon: "bi-chat-left-text-fill", subCategoryText: "Community Forum/Blog" },
  { id: "robertmiles", name: "Robert Miles (AI Safety YouTube)", url: "https://robertskmiles.com/", category: "Info/Media", importance: 3, description: "Popular YouTube channel explaining AI safety concepts clearly. Excellent introductory resource.", subCategoryIcon: "bi-youtube", subCategoryText: "YouTube Channel" },
  { id: "importai", name: "Import AI (Newsletter)", url: "https://importai.substack.com/", category: "Info/Media", importance: 3, description: "Highly regarded newsletter covering AI research, policy, and industry news (by Jack Clark/Anthropic).", subCategoryIcon: "bi-newspaper", subCategoryText: "Newsletter" },
  { id: "aisafetyinfo", name: "AI Safety Info", url: "https://aisafety.info/", category: "Info/Media", importance: 3, description: "Comprehensive resource directory for learning about AI safety.", subCategoryIcon: "bi-info-circle", subCategoryText: "Resource Directory" },
  { id: "aisafetycom", name: "AISafety.com", url: "https://www.aisafety.com/", category: "Info/Media", importance: 3, description: "Hub for AI safety resources, including guides, job board, event listings, funder lists.", subCategoryIcon: "bi-compass", subCategoryText: "Resource Hub" },
  { id: "aiscomstayinformed", name: "AISafety.com: Stay informed", url: "https://www.aisafety.com/stay-informed", category: "Info/Media", importance: 3, description: "Directory of key information sources (blogs, newsletters, podcasts, video) to keep up with AI safety.", subCategoryIcon: "bi-collection-play", subCategoryText: "Resource Directory (Media)" },
  { id: "aifrontiers", name: "AI Frontiers", url: "https://www.ai-frontiers.org/", category: "Info/Media", importance: 2, description: "Platform from CAIS with articles by experts discussing AI impacts.", subCategoryIcon: "bi-pencil-fill", subCategoryText: "Blog Platform" },
  { id: "aipolicyweekly", name: "AI Policy Weekly (Newsletter)", url: "https://aipolicyus.substack.com/", category: "Info/Media", importance: 2, description: "Weekly newsletter from CAIP on key AI policy developments.", subCategoryIcon: "bi-newspaper", subCategoryText: "Newsletter" },
  { id: "aiseventsnews", name: "AI Safety Events & Training (Newsletter)", url: "https://aisafety.com/events-and-training", category: "Info/Media", importance: 2, description: "Weekly newsletter listing newly-announced AI safety events and training programs.", subCategoryIcon: "bi-calendar-event", subCategoryText: "Newsletter (Events/Training)" },
  { id: "aischinanews", name: "AI Safety in China (Newsletter)", url: "https://aisafetychina.substack.com/", category: "Info/Media", importance: 2, description: "Updates on AI safety developments in China from Concordia AI.", subCategoryIcon: "bi-newspaper", subCategoryText: "Newsletter (Regional Focus)" },
  { id: "aiwatch", name: "AI Watch", url: "https://aiwatch.issarice.com/", category: "Info/Media", importance: 2, description: "Database tracking people, orgs, and \"products\" in the AI safety community.", subCategoryIcon: "bi-database", subCategoryText: "Database" },
  { id: "milessubstack", name: "Miles's Substack (Blog)", url: "https://milesbrundage.substack.com/", category: "Info/Media", importance: 2, description: "Blog by AI policy researcher Miles Brundage on AI evolution and governance needs.", subCategoryIcon: "bi-pencil-fill", subCategoryText: "Blog" },
  { id: "thecompendium", name: "The Compendium", url: "https://www.thecompendium.ai/", category: "Info/Media", importance: 2, description: "Living document explaining AGI race/risks for non-technical readers.", subCategoryIcon: "bi-journal-text", subCategoryText: "Living Document" }, // Moved from Policy
  { id: "obsoleteblog", name: "Obsolete (Blog)", url: "https://garrisonlovely.substack.com/", category: "Info/Media", importance: 1, description: "Publication by Garrison Lovely on the intersection of capitalism, geopolitics, and AI.", subCategoryIcon: "bi-pencil-fill", subCategoryText: "Blog" },
  { id: "powerlawblog", name: "The Power Law (Blog)", url: "https://peterwildeford.substack.com/", category: "Info/Media", importance: 1, description: "Top forecaster Peter Wildeford discusses AI, national security, innovation, etc.", subCategoryIcon: "bi-pencil-fill", subCategoryText: "Blog" },
  { id: "aistakesblog", name: "AI Safety Takes (Blog)", url: "https://substack.com/@dpaleka", category: "Info/Media", importance: 1, description: "Blog by researcher Daniel Paleka on AI safety news (posts every ~2 months).", subCategoryIcon: "bi-pencil-fill", subCategoryText: "Blog" },
  { id: "eadomains", name: "Effective Altruism Domains", url: "https://ea.domains/", category: "Info/Media", importance: 1, description: "Directory of domains available for high-impact projects.", subCategoryIcon: "bi-globe", subCategoryText: "Resource Directory" },
  { id: "govmap", name: "Governance Map", url: "https://aigov.world/", category: "Info/Media", importance: 1, description: "Cartoon map displaying key organizations/projects in global AI governance.", subCategoryIcon: "bi-map", subCategoryText: "Visualization" },
  { id: "forhumanitypod", name: "For Humanity (Podcast)", url: "https://www.youtube.com/@ForHumanityAIRisk", category: "Info/Media", importance: 1, description: "Aims to speak to regular, non-tech people about the existential threat AGI poses.", subCategoryIcon: "bi-mic-fill", subCategoryText: "Podcast" },
  { id: "siliconversations", name: "Siliconversations (YouTube)", url: "https://www.youtube.com/@Siliconversations", category: "Info/Media", importance: 1, description: "Explains AI safety concepts through entertaining stickman videos.", subCategoryIcon: "bi-youtube", subCategoryText: "Video Channel" },
  { id: "insideview", name: "The Inside View (YouTube)", url: "https://www.youtube.com/c/TheInsideView", category: "Info/Media", importance: 1, description: "Interviews, explainers, fictional threat models, paper walk-throughs.", subCategoryIcon: "bi-youtube", subCategoryText: "Video Channel" },

  // Funding
  { id: "openphil", name: "Open Philanthropy (OP)", url: "https://www.openphilanthropy.org/", category: "Funding", importance: 5, description: "The largest funder in the existential risk space, highly influential in AI safety funding.", subCategoryIcon: "bi-bank2", subCategoryText: "Major Funder" },
  { id: "sff", name: "Survival and Flourishing Fund (SFF)", url: "http://survivalandflourishing.fund/", category: "Funding", importance: 4, description: "The second largest funder in AI safety, uses a unique allocation process.", subCategoryIcon: "bi-bank2", subCategoryText: "Major Funder" },
  { id: "ltff", name: "Long-Term Future Fund (LTFF)", url: "https://funds.effectivealtruism.org/funds/far-future", category: "Funding", importance: 4, description: "Major Effective Altruism fund making grants addressing global catastrophic risks, including AI.", subCategoryIcon: "bi-bank2", subCategoryText: "EA Fund" },
  { id: "longview", name: "Longview Philanthropy", url: "https://www.longview.org/", category: "Funding", importance: 3, description: "Devises and executes bespoke giving strategies for major donors in the space.", subCategoryIcon: "bi-person-vcard", subCategoryText: "Donor Advisory" },
  { id: "gwwc", name: "Giving What We Can (GWWC)", url: "https://www.givingwhatwecan.org/", category: "Funding", importance: 3, description: "Community of donors pledged to give significantly to effective charities (incl. AI safety).", subCategoryIcon: "bi-people-fill", subCategoryText: "Donor Community" },
  { id: "manifund", name: "Manifund", url: "https://manifund.org/", category: "Funding", importance: 3, description: "Marketplace for new charities/projects (incl. AI safety), allowing impact certificate purchases.", subCategoryIcon: "bi-shop", subCategoryText: "Impact Marketplace" },
  { id: "flifellowships", name: "Future of Life Institute (FLI): Fellowships", url: "https://futureoflife.org/our-work/grantmaking-work/fellowships/", category: "Funding", importance: 3, description: "Offers PhD/postdoc fellowships in technical AI safety and US-China AI governance.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Fellowship Funding" },
  { id: "armfund", name: "AI Risk Mitigation (ARM) Fund", url: "https://www.airiskfund.com/", category: "Funding", importance: 2, description: "Grants for technical research, policy, and training to reduce catastrophic AI risks.", subCategoryIcon: "bi-bank2", subCategoryText: "Grantmaking Fund" },
  { id: "ai2050", name: "AI2050 (Schmidt Futures)", url: "https://www.schmidtfutures.com/our-work/ai2050/", category: "Funding", importance: 2, description: "Supports researchers on key AI opportunities/problems (proposals by invite only).", subCategoryIcon: "bi-bank2", subCategoryText: "Philanthropic Initiative" },
  { id: "aiscomfunders", name: "AISafety.com: Funders Directory", url: "https://www.aisafety.com/funders", category: "Funding", importance: 2, description: "Directory of financial support sources for AI safety projects (grants, VCs).", subCategoryIcon: "bi-compass", subCategoryText: "Resource Directory" },
  { id: "clrfund", name: "Center on Long-Term Risk (CLR): Fund", url: "https://longtermrisk.org/grantmaking/", category: "Funding", importance: 2, description: "Supports projects addressing worst-case suffering risks from advanced AI.", subCategoryIcon: "bi-bank2", subCategoryText: "Grantmaking Fund" },
  { id: "caif", name: "Cooperative AI Foundation (CAIF)", url: "https://www.cooperativeai.com/foundation", category: "Funding", importance: 2, description: "Supports research into improving cooperative intelligence of advanced AI.", subCategoryIcon: "bi-bank2", subCategoryText: "Charity Foundation" },
  { id: "donationslist", name: "Donations List Website", url: "https://donations.vipulnaik.com/?cause_area_filter=AI+safety", category: "Funding", importance: 2, description: "Site focused on understanding donations, donors, donees in effective altruism.", subCategoryIcon: "bi-database", subCategoryText: "Donation Tracking" },
  { id: "eaif", name: "EA Infrastructure Fund (EAIF)", url: "https://funds.effectivealtruism.org/funds/ea-community", category: "Funding", importance: 2, description: "Aims to increase impact of EA projects via access to talent, capital, knowledge.", subCategoryIcon: "bi-bank2", subCategoryText: "EA Fund" },
  { id: "ergoimpact", name: "Ergo Impact", url: "https://ergoimpact.org/", category: "Funding", importance: 2, description: "Helps major donors find, fund, and scale solutions to pressing problems.", subCategoryIcon: "bi-person-vcard", subCategoryText: "Donor Advisory" },
  { id: "foresightfunding", name: "Foresight Institute: Funding", url: "https://foresight.org/ai-safety/", category: "Funding", importance: 2, description: "Funds projects in research automation, security tech, neurotech, safe AI scenarios.", subCategoryIcon: "bi-bank2", subCategoryText: "Grantmaking" },
  { id: "flf", name: "Future of Life Foundation (FLF)", url: "https://www.flf.org/", category: "Funding", importance: 2, description: "Accelerator aiming to steer transformative technology towards benefiting life.", subCategoryIcon: "bi-rocket-takeoff", subCategoryText: "Accelerator/Funder" },
  { id: "nonlinearnetwork", name: "Nonlinear Network", url: "https://www.nonlinear.org/network", category: "Funding", importance: 2, description: "Funder network for AI existential risk reduction; connects applications with donors.", subCategoryIcon: "bi-diagram-3", subCategoryText: "Funder Network" },
  { id: "nsfsafesystems", name: "U.S. National Science Foundation (NSF): Safe Learning-Enabled Systems", url: "https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems", category: "Funding", importance: 2, description: "Funds research into design/implementation of safe learning-enabled systems.", subCategoryIcon: "bi-flag-fill", subCategoryText: "Government Funding" },
  { id: "aegrants", name: "AE Grants", url: "https://research.ae.studio/", category: "Funding", importance: 1, description: "Funds innovators/scientists creating responsible AI to increase human agency.", subCategoryIcon: "bi-bank2", subCategoryText: "Grantmaking" },
  { id: "aiscomdonationguide", name: "AISafety.com: Donation Guide", url: "https://www.aisafety.com/donation-guide", category: "Funding", importance: 1, description: "Regularly updated guide on how to donate most effectively.", subCategoryIcon: "bi-compass", subCategoryText: "Resource/Guide" },
  { id: "aria", name: "Advanced Research + Invention Agency (ARIA)", url: "https://www.aria.org.uk/", category: "Funding", importance: 1, description: "UK R&D funding agency for scientific/technological breakthroughs.", subCategoryIcon: "bi-flag-fill", subCategoryText: "Government Funding (UK)" },
  { id: "fundingsituation", name: "An Overview of the AI Safety Funding Situation", url: "https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation", category: "Funding", importance: 1, description: "Analysis of funding sources over time; useful background context.", subCategoryIcon: "bi-file-earmark-bar-graph", subCategoryText: "Analysis/Report" },
  { id: "givewiki", name: "GiveWiki", url: "https://givewiki.org/", category: "Funding", importance: 1, description: "Crowdsourced charity evaluation and philanthropic networking platform.", subCategoryIcon: "bi-database", subCategoryText: "Platform" },
  { id: "lionheart", name: "Lionheart Ventures", url: "https://www.lionheart.vc/", category: "Funding", importance: 1, description: "VC firm investing in ethical founders developing transformative technologies.", subCategoryIcon: "bi-building", subCategoryText: "Venture Capital" },
  { id: "macroscopic", name: "Macroscopic Ventures", url: "https://macroscopic.org/", category: "Funding", importance: 1, description: "Swiss nonprofit focused on reducing suffering risks (incl. AI misuse/conflict).", subCategoryIcon: "bi-bank2", subCategoryText: "Nonprofit Funder (Swiss)" },
  { id: "metacharity", name: "Meta Charity Funders", url: "https://www.metacharityfunders.com/", category: "Funding", importance: 1, description: "Network funding meta-charitable projects (often cross-cause area).", subCategoryIcon: "bi-people-fill", subCategoryText: "Donor Network" },
  { id: "mythos", name: "Mythos Ventures", url: "https://www.mythos.vc/", category: "Funding", importance: 1, description: "VC investing in teams building safe AI systems scalable to post-AGI.", subCategoryIcon: "bi-building", subCategoryText: "Venture Capital" },
  { id: "shfhs", name: "Saving Humanity from Homo Sapiens (SHfHS)", url: "http://shfhs.org/", category: "Funding", importance: 1, description: "Small organization funding work preventing human-created x-risks.", subCategoryIcon: "bi-bank2", subCategoryText: "Small Funder" },
  { id: "navfund", name: "The Navigation Fund", url: "https://www.navigation.org/", category: "Funding", importance: 1, description: "Offers grants to high-impact organizations/projects making significant changes.", subCategoryIcon: "bi-bank2", subCategoryText: "Grantmaking Fund" },

  // Field Building
  { id: "80k", name: "80,000 Hours Career Guide", url: "https://80000hours.org/problem-profiles/artificial-intelligence/", category: "Field Building", importance: 4, description: "Essential reading for planning an impactful career in the AI safety space.", subCategoryIcon: "bi-compass", subCategoryText: "Career Guide" },
  { id: "aiss", name: "AI Safety Support (AISS)", url: "https://www.aisafetysupport.org/", category: "Field Building", importance: 3, description: "Provides free one-on-one support calls for people exploring AI safety careers.", subCategoryIcon: "bi-person-hearts", subCategoryText: "Support Organization" },
  { id: "bluedot", name: "BlueDot Impact", url: "https://www.bluedotimpact.com/", category: "Field Building", importance: 3, description: "Runs online courses and career services for professionals aiming for high-impact careers (incl. AI safety).", subCategoryIcon: "bi-mortarboard", subCategoryText: "Training/Courses" },
  { id: "apart", name: "Apart Research", url: "https://www.apart-research.com/", category: "Field Building", importance: 3, description: "Trains promising researchers for independent work on AI alignment via fellowships/programs.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Research Training" },
  { id: "aisc", name: "AI Safety Camp (AISC)", url: "https://aisafety.camp/", category: "Field Building", importance: 2, description: "Runs residential programs helping aspiring researchers produce publishable alignment work.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Research Program" },
  { id: "era", name: "ERA: Evaluating Research Aptitude", url: "https://era.gregorygundersen.com/", category: "Field Building", importance: 2, description: "Helps evaluate/develop technical AI safety research aptitude (previously MLAB).", subCategoryIcon: "bi-mortarboard", subCategoryText: "Training/Evaluation" },
  { id: "aiscommunity", name: "AI Safety Community", url: "https://www.aisafety.community/", category: "Field Building", importance: 2, description: "Lists local AI safety groups, online communities, student groups etc.", subCategoryIcon: "bi-people-fill", subCategoryText: "Community Directory" },
  { id: "aisst", name: "AI Safety Student Teams (AISST)", url: "https://www.aistudents.org/", category: "Field Building", importance: 2, description: "Organization supporting university students interested in AI safety.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Student Network" },
  { id: "aisscholars", name: "AI Safety Scholars (AISS)", url: "https://sites.google.com/view/aiss-course/home", category: "Field Building", importance: 2, description: "Introductory summer research program for students (run by CAIS).", subCategoryIcon: "bi-mortarboard", subCategoryText: "Training Program" },
  { id: "axrp", name: "AI Safety Research Program (AXRP)", url: "https://axrp.net/", category: "Field Building", importance: 2, description: "Podcast interviewing AI safety researchers about their work. Useful for learning research directions.", subCategoryIcon: "bi-mic-fill", subCategoryText: "Podcast/Interviews" }, // Also Info/Media
  { id: "aisschool", name: "AI Safety School", url: "https://www.aisschool.org/", category: "Field Building", importance: 1, description: "Runs programs in the Netherlands introducing people to AI safety.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Training/Community (Netherlands)" },
  { id: "aisi", name: "AI Safety Intro Fellowship (AISI)", url: "https://aisafetyintro.com/", category: "Field Building", importance: 1, description: "Virtual introductory program (4 weeks) aimed at university students/recent grads.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Introductory Program" },
  { id: "eacourses", name: "EA Introductory Courses", url: "https://courses.effectivealtruism.org/", category: "Field Building", importance: 1, description: "Directory of various introductory courses related to Effective Altruism, often including AI Safety tracks.", subCategoryIcon: "bi-book", subCategoryText: "Course Directory" },
  { id: "eaglobal", name: "EA Global Conferences", url: "https://www.eaglobal.org/", category: "Field Building", importance: 1, description: "Major conferences for the Effective Altruism community, significant AI safety presence.", subCategoryIcon: "bi-calendar-event", subCategoryText: "Conferences" },
  { id: "serimats", name: "SERI MATS Scholars Program", url: "https://serimats.org/", category: "Field Building", importance: 1, description: "Mentored Alignment Theory Scholars program training researchers in theoretical alignment.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Research Training" },
  { id: "effectiveideas", name: "Effective Ideas", url: "https://www.effectiveideas.com/", category: "Field Building", importance: 1, description: "Platform for sharing ideas related to Effective Altruism.", subCategoryIcon: "bi-lightbulb", subCategoryText: "Blog Platform" }, // Also Info/Media
  { id: "aiscareersnews", name: "AI Safety Careers Newsletter", url: "https://aisafetycareers.substack.com/", category: "Field Building", importance: 1, description: "Quarterly newsletter highlighting resources for finding/pursuing an AI safety career.", subCategoryIcon: "bi-newspaper", subCategoryText: "Newsletter" },
  { id: "aisa", name: "AI Safety Asia (AISA)", url: "https://aisafety.asia/", category: "Field Building", importance: 1, description: "Connects junior researchers/civil servants from SE Asia with senior AI safety researchers.", subCategoryIcon: "bi-people-fill", subCategoryText: "Regional Network (Asia)" },
  { id: "aishungary", name: "AI Safety Hungary", url: "https://www.aishungary.com/", category: "Field Building", importance: 1, description: "Supports students/professionals in Hungary contributing to safe AI via seminars etc.", subCategoryIcon: "bi-flag", subCategoryText: "National Group (Hungary)" },
  { id: "arcadia", name: "Arcadia Impact", url: "https://www.arcadiaimpact.org/", category: "Field Building", importance: 1, description: "Runs various projects aimed at education, skill development, pathways into impact careers.", subCategoryIcon: "bi-mortarboard", subCategoryText: "Education/Skills" },
  { id: "meridian", name: "Meridian", url: "https://www.meridiancambridge.org/", category: "Field Building", importance: 1, description: "Coworking space and field-building nonprofit in Cambridge, UK (houses CAISH, ERA:AI Fellowship).", subCategoryIcon: "bi-building", subCategoryText: "Coworking/Field Building (UK)" }, // Also Community/Infra

  // Community/Infra
  { id: "lightcone", name: "Lightcone Infrastructure", url: "https://www.lightconeinfrastructure.com/", category: "Community/Infra", importance: 4, description: "Nonprofit maintaining LessWrong, Alignment Forum, and Lighthaven (event space).", subCategoryIcon: "bi-hdd-stack-fill", subCategoryText: "Infrastructure Org" },
  { id: "beri", name: "Berkeley Existential Risk Initiative (BERI)", url: "https://existence.org/", category: "Community/Infra", importance: 3, description: "Provides flexible funding/operations support to university x-risk research groups.", subCategoryIcon: "bi-bank", subCategoryText: "University Support Org" },
  { id: "constellation", name: "Constellation", url: "https://www.constellation.org/", category: "Community/Infra", importance: 3, description: "Center for collaborative research in AI safety (fellowships, workshops, hosting).", subCategoryIcon: "bi-building", subCategoryText: "Research Hub/Coworking" },
  { id: "enais", name: "European Network for AI Safety (ENAIS)", url: "https://enais.co/", category: "Community/Infra", importance: 2, description: "Community of researchers/policymakers across Europe advancing AI safety.", subCategoryIcon: "bi-globe-europe-africa", subCategoryText: "Regional Network (Europe)" },
  { id: "lisa", name: "London Initiative for Safe AI (LISA)", url: "https://www.safeai.org.uk/", category: "Community/Infra", importance: 2, description: "Coworking space hosting organizations, acceleration programs, and researchers.", subCategoryIcon: "bi-building", subCategoryText: "Coworking/Hub (London)" },
  { id: "aed", name: "Alignment Ecosystem Development (AED)", url: "https://alignment.dev/", category: "Community/Infra", importance: 2, description: "Builds/maintains key online resources for the AI safety community (incl. AISafety.com).", subCategoryIcon: "bi-gear", subCategoryText: "Infrastructure/Support" },
  { id: "ashgro", name: "Ashgro", url: "https://www.ashgro.org/", category: "Community/Infra", importance: 1, description: "Provides fiscal sponsorship to AI safety projects.", subCategoryIcon: "bi-umbrella-fill", subCategoryText: "Fiscal Sponsorship" },
  { id: "catalyze", name: "Catalyze Impact", url: "https://catalyze-impact.org", category: "Community/Infra", importance: 1, description: "Incubates early-stage AI safety research organizations (co-founder matching, mentorship, funding).", subCategoryIcon: "bi-rocket", subCategoryText: "Incubator" },
  { id: "ceealar", name: "Centre for Enabling EA Learning & Research (CEEALAR aka EA Hotel)", url: "https://www.ceealar.org/", category: "Community/Infra", importance: 1, description: "Free/subsidised accommodation in Blackpool, UK, for people working on GCRs.", subCategoryIcon: "bi-house-door-fill", subCategoryText: "Accommodation" },
  { id: "chinaainetwork", name: "China AI Development and Safety Network", url: "https://ai-development-and-safety-network.cn/", category: "Community/Infra", importance: 1, description: "Network for exchange/cooperation between Chinese institutions and global AI research.", subCategoryIcon: "bi-diagram-3", subCategoryText: "National Network (China)" },
  { id: "futurematters", name: "Future Matters", url: "https://future-matters.org/", category: "Community/Infra", importance: 1, description: "Provides strategy consulting services for advancing AI safety via policy, politics etc.", subCategoryIcon: "bi-briefcase", subCategoryText: "Consulting" },
  { id: "impactops", name: "Impact Ops", url: "https://impact-ops.org/", category: "Community/Infra", importance: 1, description: "Provides consultancy/hands-on support to help high-impact organizations upgrade operations.", subCategoryIcon: "bi-sliders", subCategoryText: "Operational Support" },
  { id: "nonlinear", name: "Nonlinear", url: "https://www.nonlinear.org/", category: "Community/Infra", importance: 1, description: "\"Means-neutral\" org; offers bounties, runs funder network, miscellaneous projects.", subCategoryIcon: "bi-question-circle", subCategoryText: "Miscellaneous Org" },
  { id: "saif", name: "Safe AI Forum (SAIF)", url: "https://saif.org/", category: "Community/Infra", importance: 1, description: "Fosters responsible AI governance via shared understanding/collaboration among key global actors.", subCategoryIcon: "bi-people-fill", subCategoryText: "Collaboration Forum" },
  { id: "thirdopinion", name: "Third Opinion", url: "https://third-opinion.org/", category: "Community/Infra", importance: 1, description: "Helps concerned individuals at AI frontier get expert opinions anonymously/securely.", subCategoryIcon: "bi-shield-lock-fill", subCategoryText: "Anonymous Advice" },

  // Forecasting
  { id: "aiimpacts", name: "AI Impacts", url: "https://aiimpacts.org/", category: "Forecasting", importance: 4, description: "Answers decision-relevant questions about AI's future (research, wiki, expert surveys). Key forecasting resource.", subCategoryIcon: "bi-graph-up", subCategoryText: "Research/Wiki (Forecasting)" },
  { id: "metaculus", name: "Metaculus", url: "https://www.metaculus.com/questions/?topic=ai", category: "Forecasting", importance: 3, description: "Well-calibrated forecasting platform covering many topics, including influential AI questions.", subCategoryIcon: "bi-bar-chart-line-fill", subCategoryText: "Prediction Platform" },
  { id: "epochaiforecast", name: "Epoch AI", url: "https://epochai.org/", category: "Forecasting", importance: 3, description: "Investigates key trends and questions shaping AI trajectory (compute, algorithms etc.).", subCategoryIcon: "bi-graph-up", subCategoryText: "Research Institute (Trends & Governance)" }, // Duplicate from Policy? Keep here as primary focus is trends/forecasting
  { id: "manifold", name: "Manifold Markets", url: "https://manifold.markets/", category: "Forecasting", importance: 2, description: "Prediction market platform using play money (\"mana\"); includes many AI/AI safety markets.", subCategoryIcon: "bi-bar-chart-line-fill", subCategoryText: "Prediction Platform (Play Money)" },
  { id: "quri", name: "Quantified Uncertainty Research Institute (QURI)", url: "https://quantifieduncertainty.org/", category: "Forecasting", importance: 2, description: "Advances forecasting/epistemics for long-term future; writes research/software.", subCategoryIcon: "bi-calculator-fill", subCategoryText: "Research Institute (Forecasting/Epistemics)" },
  { id: "aifutures", name: "AI Futures Project", url: "https://ai-futures.org/", category: "Forecasting", importance: 1, description: "Small group forecasting AI's future (created 'AI 2027' scenario).", subCategoryIcon: "bi-graph-up", subCategoryText: "Research Group" },
  { id: "fri", name: "Forecasting Research Institute (FRI)", url: "https://forecastingresearch.org", category: "Forecasting", importance: 1, description: "Advancing forecasting science; works with policymakers/nonprofits.", subCategoryIcon: "bi-calculator-fill", subCategoryText: "Research Institute (Forecasting Science)" },
  { id: "mitfuturetech", name: "MIT FutureTech", url: "https://futuretech.mit.edu/", category: "Forecasting", importance: 1, description: "Identifies/understands computing trends affecting economic growth/risk.", subCategoryIcon: "bi-bank", subCategoryText: "University Group (MIT)" },
  { id: "tfi", name: "Transformative Futures Institute (TFI)", url: "https://transformative.org/", category: "Forecasting", importance: 1, description: "Explores underutilized foresight methods to anticipate societal-scale AI risks.", subCategoryIcon: "bi-binoculars", subCategoryText: "Research Institute (Foresight Methods)" },

  // Inactive
  { id: "fhi", name: "Future of Humanity Institute (FHI)", url: "https://www.fhi.ox.ac.uk/", category: "Inactive", importance: 1, description: "Highly influential longtermist/x-risk research organization led by Nick Bostrom. Closed 2024.", subCategoryIcon: "bi-slash-circle", subCategoryText: "University Center (Oxford, Inactive)" },
  { id: "aiscc", name: "AI Safety Communications Centre (AISCC)", url: "https://aiscc.org/", category: "Inactive", importance: 1, description: "Connected journalists to AI safety experts and resources.", subCategoryIcon: "bi-slash-circle", subCategoryText: "Comms Support (Inactive)" },
  { id: "ash", name: "AI Safety Hub (ASH)", url: "https://www.aisafetyhub.org/", category: "Inactive", importance: 1, description: "Ran AI Safety Hub Labs (now run by LASR Labs).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Training Program (Superseded)" },
  { id: "cas", name: "Campaign for AI Safety (CAS)", url: "http://campaignforaisafety.org/", category: "Inactive", importance: 1, description: "Increased public understanding, called for laws (merged with ERO).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Campaign Group (Merged)" },
  { id: "mlaisafetyupdates", name: "ML & AI Safety Updates", url: "https://www.alignmentforum.org/posts/uRosq4YtNiZxywcAq/newsletter-for-alignment-research-the-ml-safety-updates", category: "Inactive", importance: 1, description: "Weekly podcast, YouTube, newsletter with updates (dropped).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Media (Inactive)" },
  { id: "openbook", name: "OpenBook", url: "https://openbook.fyi/", category: "Inactive", importance: 1, description: "Database of grants in effective altruism (no longer maintained).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Database (Inactive)" },
  { id: "aialignmentawards", name: "AI Alignment Awards", url: "https://www.alignmentawards.com/", category: "Inactive", importance: 1, description: "Ran research paper/essay-writing contests (last round 2023).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Contest (Inactive)" },
  { id: "aisafetyideas", name: "AI Safety Ideas", url: "https://aisafetyideas.com/", category: "Inactive", importance: 1, description: "Crowdsourced repository of research projects (run by Apart Research).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Repository (Inactive)" },
  { id: "aissnewsletter", name: "AI Safety Support (AISS) Newsletter", url: "https://www.aisafetysupport.org/newsletter", category: "Inactive", importance: 1, description: "Listed opportunities in AI alignment (dropped).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Newsletter (Inactive)" },
  { id: "aizi", name: "From AI to ZI (Blog)", url: "https://aizi.substack.com/", category: "Inactive", importance: 1, description: "Blog by Robert Huben (currently semi-dormant).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Blog (Semi-Dormant)" },
  { id: "genink", name: "generative.ink (Blog)", url: "https://generative.ink/posts/", category: "Inactive", importance: 1, description: "The blog of janus the GPT cyborg (last post 2023).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Blog (Inactive)" },
  { id: "lightspeedgrants", name: "Lightspeed Grants", url: "https://lightspeedgrants.org/", category: "Inactive", importance: 1, description: "Fast funding for trajectory-changing projects (last round 2023).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Grantmaking (Inactive)" },
  { id: "preamblefoundation", name: "Preamble Windfall Foundation", url: "https://www.preambleforgood.org/", category: "Inactive", importance: 1, description: "Funding organization aiming to minimize AI risk (inactive).", subCategoryIcon: "bi-slash-circle", subCategoryText: "Funding Org (Inactive)" },
  { id: "stopai", name: "Stop AGI", url: "http://stop.ai/", category: "Inactive", importance: 1, description: "Website communicating risks and prevention proposals.", subCategoryIcon: "bi-slash-circle", subCategoryText: "Website/Advocacy (Inactive)" },
  { id: "superlinear", name: "Superlinear Prizes", url: "https://www.super-linear.org/", category: "Inactive", importance: 1, description: "Decentralized bounty platform for x-risk reduction/EA causes.", subCategoryIcon: "bi-slash-circle", subCategoryText: "Bounty Platform (Inactive)" }
];

// --- END OF FILE data.js ---