<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- SEO Metadata -->
<title>Yudkowsky: Rationality, AI &amp; The Sequences Cheatsheet</title>
<meta content="Immersive cheatsheet on Eliezer Yudkowsky's core ideas: rationality, AI alignment, P(doom), optimization power, Orthogonality Thesis, and an in-depth guide to 'The Sequences' from LessWrong on Bayesian reasoning, cognitive biases, epistemology, and AI safety. Features comprehensive tooltips." name="description"/>
<meta content="Eliezer Yudkowsky, LessWrong Sequences, AI alignment, Rationality, P(doom), Existential risk AI, Optimization Power, Search Spaces, Cognitive biases, Bayesian reasoning, Epistemology, Map and Territory, Rationality From AI to Zombies, Instrumental Convergence, Orthogonality Thesis, MIRI, AI Safety, Value Theory, Metaethics, Synaptic Flow, Tech Philosophy, Effective Altruism, Tooltips" name="keywords"/>
<link href="https://cheatsheets.davidveksler.com/yudkowsky-rationality-ai-cheatsheet.html" rel="canonical"/>
<link href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;ðŸ§ &lt;/text&gt;&lt;/svg&gt;" rel="icon"/>
<!-- Bootstrap CSS -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"/>
<!-- Bootstrap Icons -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css" rel="stylesheet"/>
<!-- Google Fonts -->
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;600;700&amp;family=Inter:wght@400;500;700&amp;display=swap" rel="stylesheet"/>
<style>
        :root {
            --bg-color-deep: #0d1117; /* Deep space blue/charcoal */
            --bg-color-medium: #161b22; /* Slightly lighter for cards/elements */
            --bg-color-light: #21262d; /* For subtle contrasts or hover states */

            --text-color-primary: #f0f6fc; /* Off-white for headings, bright text */
            --text-color-secondary: #c9d1d9; /* Light cool grey for body */
            --text-color-muted: #8b949e; /* Muted grey for less emphasis */

            --accent-primary: #58a6ff; /* Vibrant, glowing blue */
            --accent-primary-glow: rgba(88, 166, 255, 0.3);
            --accent-primary-darker: #388bfd;

            --accent-secondary: #ae81ff; /* Muted magenta/purple */
            --accent-secondary-glow: rgba(174, 129, 255, 0.2);

            --border-color: #30363d; /* Subtle border for elements */
            --border-color-accent: var(--accent-primary);

            --font-headings: 'Fira Code', monospace;
            --font-body: 'Inter', sans-serif;

            --shadow-color-primary: rgba(88, 166, 255, 0.2);
            --shadow-color-secondary: rgba(0,0,0, 0.5);

            /* Tooltip Specific Variables (inspired by airisk.html and adapted) */
            --tooltip-bg: var(--bg-color-light);
            --tooltip-color: var(--text-color-secondary);
            --tooltip-link-color: var(--accent-primary);
            --tooltip-link-hover-color: var(--accent-primary-darker);
            --tooltip-border-color: var(--border-color); /* Optional border for tooltip */
        }

        @keyframes subtleGradientFlow {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        @keyframes pulseGlow {
            0% { box-shadow: 0 0 8px var(--accent-primary-glow), inset 0 0 5px rgba(88, 166, 255, 0.1); }
            50% { box-shadow: 0 0 16px var(--accent-primary-glow), inset 0 0 8px rgba(88, 166, 255, 0.2); }
            100% { box-shadow: 0 0 8px var(--accent-primary-glow), inset 0 0 5px rgba(88, 166, 255, 0.1); }
        }

        body {
            font-family: var(--font-body);
            background-color: var(--bg-color-deep);
            background: linear-gradient(280deg, var(--bg-color-deep), #010409, var(--bg-color-deep));
            background-size: 400% 400%;
            animation: subtleGradientFlow 25s ease infinite;
            color: var(--text-color-secondary);
            line-height: 1.75;
            margin: 0;
            padding: 0;
            display: flex;
            flex-direction: column;
            min-height: 100vh;
            overflow-x: hidden;
        }

        .container-main {
            width: 90%;
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px 0;
        }

        .page-header {
            background: transparent;
            color: var(--text-color-primary);
            padding: 4rem 1.5rem;
            text-align: center;
            margin-bottom: 3rem;
            border-bottom: 1px solid var(--border-color);
        }

        .page-header h1 {
            font-family: var(--font-headings);
            margin: 0;
            font-size: 3.2rem;
            font-weight: 700;
            letter-spacing: 1.5px;
            color: var(--accent-primary);
            text-shadow: 0 0 10px var(--accent-primary-glow), 0 0 20px var(--accent-primary-glow);
        }

        .page-header .subtitle {
            font-family: var(--font-body);
            font-size: 1.3rem;
            margin-top: 1rem;
            color: var(--text-color-muted);
            font-weight: 400;
        }

        main {
            flex-grow: 1;
        }

        .schema-container {
            background-color: transparent;
            border: none;
            padding: 0;
            margin-bottom: 3.5rem;
        }

        .section-title {
            font-family: var(--font-headings);
            color: var(--accent-primary);
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: .12em;
            font-size: 1.8rem;
            padding-bottom: 1rem;
            margin-bottom: 2.5rem;
            border-bottom: 2px solid var(--accent-primary);
            text-align: left;
            text-shadow: 0 0 5px var(--accent-primary-glow);
        }
        .section-title .bi {
            margin-right: 0.75rem;
            font-size: 0.9em;
            opacity: 0.8;
        }

        .info-card {
            background: var(--bg-color-medium);
            border: 1px solid var(--border-color);
            border-left: 5px solid var(--accent-primary);
            border-radius: 8px;
            box-shadow: 0 6px 20px rgba(0,0,0,0.4), 0 0 10px var(--accent-primary-glow);
            margin-bottom: 2.5rem;
            display: flex;
            flex-direction: column;
            transition: transform 0.3s ease-out, box-shadow 0.3s ease-out, border-color 0.3s ease;
        }
        .info-card:hover {
            transform: translateY(-6px) scale(1.02);
            box-shadow: 0 10px 30px rgba(0,0,0,0.5), 0 0 20px var(--accent-primary-glow);
            border-left-color: var(--accent-primary-darker);
        }

        .info-card .card-header-content {
            padding: 0;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .info-card h5 {
            font-family: var(--font-headings);
            color: var(--text-color-primary);
            background-color: transparent;
            font-size: 1.4rem;
            text-align: left;
            margin: 0;
            padding: 1.2rem 1.5rem;
            font-weight: 600;
            border-bottom: 1px solid var(--border-color);
            border-radius: 7px 7px 0 0;
            display: flex;
            align-items: center;
        }
        .info-card h5 .bi {
            margin-right: 0.6rem;
            color: var(--accent-primary);
            font-size: 1.1em;
        }

        .card-content-wrapper {
            padding: 1.5rem;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .info-card p.summary {
            font-family: var(--font-body);
            font-size: 1rem;
            color: var(--text-color-secondary);
            margin-top: 0;
            margin-bottom: 1.25rem;
            flex-grow: 1;
        }

        .details-toggle {
            font-family: var(--font-body);
            font-size: 0.9rem;
            margin-top: auto;
            align-self: flex-start;
            padding: 0.6rem 1.2rem;
            color: var(--accent-primary);
            border: 2px solid var(--accent-primary);
            background-color: transparent;
            transition: background-color 0.25s ease, color 0.25s ease, box-shadow 0.25s ease, transform 0.15s ease;
            display: inline-flex;
            align-items: center;
            gap: 0.6em;
            border-radius: 5px;
            cursor: pointer;
            font-weight: 700;
        }
        .details-toggle:hover, .details-toggle:focus {
            background-color: var(--accent-primary);
            color: var(--bg-color-deep);
            outline: none;
            box-shadow: 0 0 12px var(--accent-primary-glow);
            transform: scale(1.03);
        }

        .details-toggle .toggle-icon {
            transition: transform 0.3s cubic-bezier(0.68, -0.55, 0.27, 1.55);
            display: inline-block;
        }
        .details-toggle[aria-expanded="true"] .toggle-icon {
            transform: rotate(180deg);
        }

        .collapse-content {
            font-family: var(--font-body);
            font-size: 0.98rem;
            padding: 0 1.5rem;
            margin-top: 0;
            color: var(--text-color-main); /* Assuming this should be --text-color-secondary or primary */
            background-color: var(--bg-color-medium);
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.45s ease-in-out, padding 0.45s ease-in-out;
            border-radius: 0 0 7px 7px;
        }
        .collapse-content.active {
            padding: 1.5rem;
            border-top: 1px solid var(--border-color);
        }

        .collapse-content h6 {
            font-family: var(--font-headings);
            font-weight: 700;
            color: var(--accent-secondary);
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            font-size: 1.15rem;
            border-left: 3px solid var(--accent-secondary);
            padding-left: 0.75rem;
        }
        .collapse-content h6:first-child {
            margin-top: 0;
        }

        .collapse-content ul {
            padding-left: 25px;
            margin-bottom: 1.25rem;
            list-style-type: none;
        }
         .collapse-content ul li {
            position: relative;
            padding-left: 1.5em;
            margin-bottom: 0.8rem;
            color: var(--text-color-secondary);
        }
        .collapse-content ul li::before {
            content: '';
            position: absolute;
            left: 0;
            top: 0.5em;
            width: 0.5em;
            height: 0.5em;
            background-color: var(--accent-primary);
            border-radius: 50%;
            box-shadow: 0 0 5px var(--accent-primary-glow);
        }
        .collapse-content ul ul {
            margin-top: 0.75rem;
        }
         .collapse-content ul ul li::before {
            background-color: var(--accent-secondary);
             box-shadow: 0 0 5px var(--accent-secondary-glow);
        }

        .collapse-content li strong {
            font-weight: 700;
            color: var(--text-color-primary);
        }

        .collapse-content p {
            margin-bottom: 1.25rem;
        }
        .collapse-content p:last-child {
            margin-bottom: 0;
        }

        .term {
            font-weight: 700;
            color: var(--accent-secondary);
            background-color: rgba(174, 129, 255, 0.1);
            padding: 0.2em 0.5em;
            border-radius: 4px;
            border-bottom: 1px solid var(--accent-secondary);
            font-family: var(--font-body);
            font-size: 0.95em;
        }

        a {
            color: var(--accent-primary);
            text-decoration: none;
            font-weight: 700;
            transition: color 0.2s ease, text-shadow 0.2s ease;
        }
        a:hover {
            color: #8afff7;
            text-decoration: none;
            text-shadow: 0 0 8px var(--accent-primary-glow);
        }

        .page-footer {
            text-align: center;
            padding: 3rem 1rem;
            background-color: var(--bg-color-medium);
            color: var(--text-color-muted);
            font-size: 0.95rem;
            margin-top: auto;
            border-top: 1px solid var(--border-color);
        }
        .page-footer a {
            color: var(--accent-primary);
        }
        .page-footer a:hover {
            color: #8afff7;
        }

        /* Custom Scrollbar */
        ::-webkit-scrollbar {
            width: 10px;
        }
        ::-webkit-scrollbar-track {
            background: var(--bg-color-deep);
        }
        ::-webkit-scrollbar-thumb {
            background: var(--accent-primary);
            border-radius: 5px;
            border: 2px solid var(--bg-color-deep);
        }
        ::-webkit-scrollbar-thumb:hover {
            background: var(--accent-primary-darker);
        }

        /* Bootstrap Row customization */
        .row {
            --bs-gutter-x: 2.5rem;
            --bs-gutter-y: 2.5rem;
        }

        /* Tooltip Styling (inspired by airisk.html and adapted for dark theme) */
        [data-bs-toggle="tooltip"] {
            cursor: help;
            text-decoration: none;
        }

        .term[data-bs-toggle="tooltip"] {
            border-bottom: 1px dotted var(--accent-primary); /* Override .term's solid border for tooltip indication */
        }
        .term[data-bs-toggle="tooltip"]:hover {
            color: var(--accent-primary); /* Change color on hover */
            border-bottom-color: var(--accent-primary); /* Ensure dotted line also changes color if desired */
        }

        span:not(.term)[data-bs-toggle="tooltip"],
        strong[data-bs-toggle="tooltip"] {
            border-bottom: 1px dotted var(--accent-primary);
            color: var(--text-color-primary); /* Or var(--accent-primary) for more pop */
            font-weight: 600; /* From airisk example, makes it stand out */
        }
        /* Ensure strong tags with tooltips retain their boldness or defined weight */
        strong[data-bs-toggle="tooltip"] {
            font-weight: 700; /* Match other strong tags or desired weight */
        }

        span:not(.term)[data-bs-toggle="tooltip"]:hover,
        strong[data-bs-toggle="tooltip"]:hover {
            color: var(--accent-primary);
            border-bottom-color: var(--accent-primary);
        }

        .tooltip {
            --bs-tooltip-bg: var(--tooltip-bg);
            --bs-tooltip-color: var(--tooltip-color);
            --bs-tooltip-max-width: 450px;
            --bs-tooltip-padding-x: 1rem;
            --bs-tooltip-padding-y: 0.75rem;
            --bs-tooltip-font-size: 0.92rem;
            --bs-tooltip-border-radius: 0.375rem; /* Bootstrap default is 0.375rem */
            /* border: 1px solid var(--tooltip-border-color); /* Optional: adds a border to the tooltip */
            z-index: 1080; /* Ensure visibility */
        }

        .tooltip-inner a {
            color: var(--tooltip-link-color);
            text-decoration: underline;
            font-weight: 600; /* Make links in tooltips slightly bolder */
        }
        .tooltip-inner a:hover {
            color: var(--tooltip-link-hover-color);
        }


        @media (max-width: 768px) {
            .page-header h1 { font-size: 2.4rem; }
            .page-header .subtitle { font-size: 1.1rem; }
            .section-title { font-size: 1.4rem; }
            .info-card h5 { font-size: 1.2rem; }
            .info-card p.summary, .collapse-content { font-size: 0.98rem; }
            .row { --bs-gutter-x: 1.5rem; }
        }
    </style>
<meta content="images/yudkowsky-rationality-ai-cheatsheet.png" property="og:image"/><meta content="images/yudkowsky-rationality-ai-cheatsheet.png" name="twitter:image"/></head>
<body>
<header class="page-header">
<h1>Yudkowsky's Core Concepts: AI, Rationality &amp; Existential Risk</h1>
<p class="subtitle">An Interactive Cheatsheet on Rationality, AI Alignment, Existential Risk, &amp; The Sequences</p>
</header>
<main class="container-main">
<div class="row">
<div class="col-12">
<div class="schema-container" data-section-id="section-introduction">
<h2 class="section-title" id="section-introduction-title"><i class="bi bi-person-workspace"></i> Introduction</h2>
<div class="row">
<div class="col-lg-12">
<div class="info-card" id="card-yudkowsky-overview">
<div class="card-header-content">
<h5><i class="bi bi-person-fill"></i> <strong data-bs-html="true" data-bs-toggle="tooltip" title="Eliezer Yudkowsky is an American AI researcher and writer known for his work on artificial intelligence and rationality. He co-founded the Machine Intelligence Research Institute (MIRI). &lt;a href='https://intelligence.org/team/#eliezer-yudkowsky' target='_blank' rel='noopener noreferrer'&gt;MIRI Profile&lt;/a&gt;">Eliezer Yudkowsky</strong>: The Thinker &amp; His Mission</h5>
<div class="card-content-wrapper">
<p class="summary">Eliezer Yudkowsky is a prominent American AI researcher, writer, and philosopher, best known for his work on decision theory and the potential risks and benefits of artificial general intelligence (AGI). He co-founded the <strong data-bs-html="true" data-bs-toggle="tooltip" title="The Machine Intelligence Research Institute (MIRI) is a non-profit research institute studying the mathematical foundations of intelligent behavior and the safety of future AI systems. &lt;a href='https://intelligence.org/' target='_blank' rel='noopener noreferrer'&gt;Visit MIRI&lt;/a&gt;">Machine Intelligence Research Institute (MIRI)</strong>.</p>
<button aria-controls="collapse-yudkowsky-overview" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-yudkowsky-overview" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse collapse-content" id="collapse-yudkowsky-overview">
<h6>Primary Concerns &amp; Contributions:</h6>
<ul>
<li><strong>Refining Human Rationality:</strong> Developing techniques and mental models to overcome <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Systematic patterns of deviation from norm or rationality in judgment. They often arise from heuristics (mental shortcuts). &lt;a href='https://www.lesswrong.com/tag/cognitive-biases' target='_blank' rel='noopener noreferrer'&gt;LessWrong on Biases&lt;/a&gt;">cognitive biases</span> and improve decision-making. His foundational writings, known as <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="A series of essays by Eliezer Yudkowsky on human rationality and artificial intelligence, originally published on LessWrong and Overcoming Bias, later compiled into the book 'Rationality: From AI to Zombies'. &lt;a href='https://www.readthesequences.com/' target='_blank' rel='noopener noreferrer'&gt;Read The Sequences&lt;/a&gt;">The Sequences</span>, were originally published on blogs like <a data-bs-html="true" data-bs-toggle="tooltip" href="https://www.overcomingbias.com/" rel="noopener noreferrer" target="_blank" title="A blog co-founded by Robin Hanson and Eliezer Yudkowsky, focusing on overcoming self-deception and improving reasoning.">Overcoming Bias</a> and <a data-bs-html="true" data-bs-toggle="tooltip" href="https://www.lesswrong.com/" rel="noopener noreferrer" target="_blank" title="A community blog and forum focused on rationality, artificial intelligence, and related topics, heavily influenced by Eliezer Yudkowsky's writings. &lt;a href='https://www.lesswrong.com/' target='_blank' rel='noopener noreferrer'&gt;Visit LessWrong&lt;/a&gt;">LessWrong</a>.</li>
<li><strong>Artificial General Intelligence (AGI):</strong> Exploring the profound societal implications of <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Artificial Superintelligence (ASI) is a hypothetical AI that possesses intelligence far surpassing that of the brightest and most gifted human minds in virtually all domains of interest. &lt;a href='https://www.coursera.org/articles/superintelligence-ai' target='_blank' rel='noopener noreferrer'&gt;More on ASI&lt;/a&gt;">superintelligent AI</span>, with a strong emphasis on potential <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Events that could cause human extinction or permanently and drastically curtail humanity's potential. Advanced AI is considered a significant x-risk. &lt;a href='https://futureoflife.org/background/existential-risk/' target='_blank' rel='noopener noreferrer'&gt;FLI on X-Risk&lt;/a&gt;">existential risks</span> if AGI is not developed safely.</li>
<li><strong>AI Safety &amp; Alignment:</strong> Pioneering research into the <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The challenge of ensuring that AI systems' goals are aligned with human values and intentions, especially as AI becomes more powerful. Misalignment could lead to unintended harmful outcomes. &lt;a href='https://www.ibm.com/topics/ai-alignment' target='_blank' rel='noopener noreferrer'&gt;IBM on AI Alignment&lt;/a&gt; [4] &lt;a href='https://www.techtarget.com/searchenterpriseai/definition/AI-alignment' target='_blank' rel='noopener noreferrer'&gt;TechTarget Definition&lt;/a&gt; [3]">AI alignment problem</span> â€“ the challenge of ensuring an AI's goals are robustly aligned with human values and intentions to prevent unintended harmful outcomes. MIRI's work focuses on this critical area.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="schema-container" data-section-id="section-rationality">
<h2 class="section-title" id="section-rationality-title"><i class="bi bi-brain"></i> Foundations of Rationality</h2>
<div class="row">
<div class="col-lg-6 col-md-12">
<div class="info-card" id="card-cognitive-biases">
<div class="card-header-content">
<h5><i class="bi bi-kanban-fill"></i> Overcoming Cognitive Biases</h5>
<div class="card-content-wrapper">
<p class="summary">Cognitive biases are systematic errors in thinking affecting decisions. Yudkowsky's <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="A collection of essays by Eliezer Yudkowsky discussing human rationality, cognitive biases, and artificial intelligence. They form the basis of the book 'Rationality: From AI to Zombies'. &lt;a href='https://www.readthesequences.com/' target='_blank' rel='noopener noreferrer'&gt;Explore The Sequences&lt;/a&gt;">The Sequences</span> stress recognizing and mitigating these for clearer thought and accurate beliefs.</p>
<button aria-controls="collapse-cognitive-biases" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-cognitive-biases" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse collapse-content" id="collapse-cognitive-biases">
<p>Biases often arise from mental shortcuts (<span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Mental shortcuts or rules of thumb that humans use to make judgments and decisions quickly and efficiently. While often useful, they can lead to systematic errors or cognitive biases.">heuristics</span>).</p>
<h6>Common Biases Explored:</h6>
<ul>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="The tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's preexisting beliefs or hypotheses.">Confirmation Bias:</strong> Favoring info confirming existing beliefs.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="A mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision.">Availability Heuristic:</strong> Overestimating easily recalled events.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="A cognitive bias where an individual depends too heavily on an initial piece of information offered (considered to be the 'anchor') when making decisions. [10, 20, 25, 34, 38]">Anchoring Bias:</strong> Over-relying on initial info.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="The failure to appreciate the massive differences in scale when evaluating problems, especially when emotional responses are involved. For instance, the suffering of one person can evoke a strong emotional response, while the suffering of thousands might not evoke a proportionally larger response. &lt;a href='https://www.lesswrong.com/tag/scope-insensitivity' target='_blank' rel='noopener noreferrer'&gt;LessWrong on Scope Insensitivity&lt;/a&gt;">Scope Insensitivity:</strong> Failing to scale emotional response to problem magnitude.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="Reasoning processes that are driven by a desire to reach a particular conclusion, rather than an objective assessment of evidence. Rationalization often involves finding plausible-sounding justifications for beliefs already held for other reasons.">Motivated Cognition/Rationalization:</strong> Reasoning towards a predetermined conclusion.</li>
</ul>
<h6>Techniques for Mitigation Advocated:</h6>
<ul>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="Actively seeking out and seriously considering evidence and arguments that contradict one's own beliefs or hypotheses. This helps to counteract confirmation bias.">Considering the Opposite:</strong> Arguing against own beliefs.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="The process of improving the accuracy of one's probabilistic judgments by getting feedback on past judgments and adjusting one's confidence levels accordingly.">Calibration Training:</strong> Improving probability assignment accuracy.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="Treating a feeling of confusion not as a personal failing, but as an important signal that one's current understanding or mental model of a situation is flawed or incomplete, prompting further investigation. &lt;a href='https://www.lesswrong.com/posts/k9dsL36SgXvtNoWFp/noticing-confusion' target='_blank' rel='noopener noreferrer'&gt;Yudkowsky on Noticing Confusion&lt;/a&gt;">Noticing Confusion:</strong> Treating confusion as a signal of flawed understanding.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="The idea that beliefs should have tangible, testable consequences or predictions about the world. If a belief doesn't constrain your expectations of future experiences, it's not 'paying rent' in your mental model. &lt;a href='https://www.lesswrong.com/posts/aH2FtPG2T8NB2qM2S/making-beliefs-pay-rent-in-anticipated-experiences' target='_blank' rel='noopener noreferrer'&gt;Yudkowsky on Beliefs Paying Rent&lt;/a&gt;">Making Beliefs "Pay Rent":</strong> Ensuring beliefs have tangible, anticipatory consequences.</li>
</ul>
</div>
</div>
</div>
<div class="col-lg-6 col-md-12">
<div class="info-card" id="card-bayesian-reasoning">
<div class="card-header-content">
<h5><i class="bi bi-calculator-fill"></i> Bayesian Reasoning</h5>
<div class="card-content-wrapper">
<p class="summary"><strong data-bs-html="true" data-bs-toggle="tooltip" title="A method of statistical inference that updates the probability of a hypothesis as more evidence or information becomes available. It is a formal way to combine new evidence with prior beliefs. [26, 33, 40, 42] &lt;a href='https://plato.stanford.edu/entries/bayes-theorem/' target='_blank' rel='noopener noreferrer'&gt;SEP on Bayes' Theorem&lt;/a&gt;">Bayesian reasoning</strong>, a cornerstone of Yudkowsky's rationality, is a formal framework for updating beliefs with new evidence, adjusting probabilities from priors to posteriors.</p>
<button aria-controls="collapse-bayesian-reasoning" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-bayesian-reasoning" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse collapse-content" id="collapse-bayesian-reasoning">
<h6>Core Idea of Bayesian Epistemology:</h6>
<p>Bayesianism quantifies logical belief shifts with new evidence:</p>
<ul>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="The initial probability assigned to a hypothesis before new evidence is considered. It represents one's degree of belief in the hypothesis based on prior knowledge. [1, 13, 18, 22]">Prior Probability (Priors):</strong> Initial belief strength before new evidence.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="The probability of observing the new evidence if a particular hypothesis is true (P(Evidence|Hypothesis)). It quantifies how well the evidence supports the hypothesis. [5]">Likelihood of Evidence:</strong> Probability of evidence given the hypothesis (and alternatives).</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="The revised probability of a hypothesis after taking new evidence into account, calculated using Bayes' Theorem. It represents the updated degree of belief. [1, 14, 15, 35]">Posterior Probability (Posteriors):</strong> Updated belief strength after incorporating evidence via <strong data-bs-html="true" data-bs-toggle="tooltip" title="A mathematical formula used to update the probability for a hypothesis as more evidence or information becomes available. It is P(H|E) = [P(E|H) * P(H)] / P(E). [1, 6, 23, 39, 41] &lt;a href='https://www.investopedia.com/terms/b/bayes-theorem.asp' target='_blank' rel='noopener noreferrer'&gt;Investopedia&lt;/a&gt; [1]">Bayes' Theorem</strong>.</li>
</ul>
<p>This refines one's mental <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="A metaphor for one's understanding, beliefs, or mental models about reality. The 'map' is not the 'territory' (reality itself) but should accurately represent it. &lt;a href='https://wiki.lesswrong.com/wiki/Map_and_territory' target='_blank' rel='noopener noreferrer'&gt;LessWrong Wiki&lt;/a&gt;">map</span> to better reflect reality (the <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="A metaphor for reality itself, as distinct from one's mental models or beliefs about it ('the map'). The goal of rationality is to make one's map accurately reflect the territory. &lt;a href='https://wiki.lesswrong.com/wiki/Map_and_territory' target='_blank' rel='noopener noreferrer'&gt;LessWrong Wiki&lt;/a&gt;">territory</span>), a key theme in "Map and Territory" from "<strong data-bs-html="true" data-bs-toggle="tooltip" title="A large collection of essays by Eliezer Yudkowsky on human rationality and artificial intelligence, compiled from The Sequences. &lt;a href='https://www.readthesequences.com/' target='_blank' rel='noopener noreferrer'&gt;Read Online&lt;/a&gt;">Rationality: From AI to Zombies</strong>."</p>
</div>
</div>
</div>
</div>
</div>
<div class="schema-container" data-section-id="section-ai-alignment">
<h2 class="section-title" id="section-ai-alignment-title"><i class="bi bi-shield-lock-fill"></i> AI Alignment &amp; Existential Risk</h2>
<div class="row">
<div class="col-lg-4 col-md-6">
<div class="info-card" id="card-alignment-problem">
<div class="card-header-content">
<h5><i class="bi bi-node-plus-fill"></i> The Alignment Problem</h5>
<div class="card-content-wrapper">
<p class="summary">The critical challenge of ensuring advanced AI, especially AGI, pursues goals genuinely aligned with human values, preventing unintended catastrophic outcomes. [2, 3, 4, 7, 30]</p>
<button aria-controls="collapse-alignment-problem" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-alignment-problem" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse collapse-content" id="collapse-alignment-problem">
<p>A superintelligent AI, even with benign programmed goals, could find destructive pathways if values aren't precisely specified. Slight misalignments, amplified by vast <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The capacity of an intelligent system to efficiently steer the future towards specific outcomes or configurations that rank high according to its goal function. &lt;a href='https://www.alignmentforum.org/tag/optimization-power' target='_blank' rel='noopener noreferrer'&gt;Alignment Forum&lt;/a&gt;">optimization power</span>, could lead to <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Risks that threaten the premature extinction of Earth-originating intelligent life or the permanent and drastic curtailment of its potential. &lt;a href='https://nickbostrom.com/existential/risks.html' target='_blank' rel='noopener noreferrer'&gt;Nick Bostrom on X-Risk&lt;/a&gt;">existential risk</span>. Specifying complex human values robustly is an extraordinary challenge.</p>
</div>
</div>
</div>
<div class="col-lg-4 col-md-6">
<div class="info-card" id="card-optimization-power">
<div class="card-header-content">
<h5><i class="bi bi-lightning-charge-fill"></i> Optimization Power &amp; Search Spaces</h5>
<div class="card-content-wrapper">
<p class="summary">AGI's capability is immense <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The ability of an AI system to efficiently find and implement solutions to achieve its goals by navigating vast possibility spaces. This power can be dangerous if the AI's goals are misaligned with human values.">optimization power</span>: efficiently searching vast <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The abstract space of all possible states, actions, or solutions that an AI can consider or explore to achieve its objectives. The size of these spaces can be astronomically large.">search spaces</span> of possibilities to find and implement solutions for its objectives. This power is transformative but also dangerous if misaligned.</p>
<button aria-controls="collapse-optimization-power" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-optimization-power" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-optimization-power">
<h6>Key Concepts:</h6>
<ul>
<li><strong>Optimization Power:</strong> Ability to steer the future into desired configurations according to a goal function.</li>
<li><strong>Search Spaces:</strong> The astronomically large set of all possible actions an AI could consider.</li>
</ul>
<p>Misaligned goals combined with superhuman optimization power can lead to extreme, unforeseen "solutions" that are technically optimal for the AI but catastrophic for humans. Such goals could be achieved quickly and irreversibly.</p>
</div>
</div>
</div>
<div class="col-lg-4 col-md-6">
<div class="info-card" id="card-orthogonality-thesis">
<div class="card-header-content">
<h5><i class="bi bi-arrows-angle-contract"></i> Orthogonality Thesis</h5>
<div class="card-content-wrapper">
<p class="summary">The <strong data-bs-html="true" data-bs-toggle="tooltip" title="Proposed by Nick Bostrom, this thesis states that an AI's level of intelligence (its capability to achieve goals) is independent of its final goals. A highly intelligent AI could pursue any arbitrary goal, not necessarily benevolent ones. [21] &lt;a href='https://wiki.lesswrong.com/wiki/Orthogonality_thesis' target='_blank' rel='noopener noreferrer'&gt;LessWrong Wiki&lt;/a&gt;">Orthogonality Thesis</strong> posits that an AI's intelligence (capability) and its final goals are independent. High intelligence does not inherently imply human-compatible goals. An AI can be superintelligent yet pursue any arbitrary goal.</p>
<button aria-controls="collapse-orthogonality-thesis" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-orthogonality-thesis" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-orthogonality-thesis">
<p>Intelligence is about effective goal achievement, not inherent morality or wisdom. We cannot expect superintelligence to "understand" what we "really mean" or converge on human values by default; values must be explicitly and correctly specified. There's no "default" benevolence.</p>
</div>
</div>
</div>
<div class="col-lg-4 col-md-6">
<div class="info-card" id="card-instrumental-convergence">
<div class="card-header-content">
<h5><i class="bi bi-tools"></i> Instrumental Convergence</h5>
<div class="card-content-wrapper">
<p class="summary"><strong data-bs-html="true" data-bs-toggle="tooltip" title="The idea that intelligent agents with a wide range of different final goals will nevertheless tend to pursue similar instrumental (sub-goals) because these sub-goals are useful for achieving almost any ultimate objective. Examples include self-preservation, resource acquisition, and cognitive enhancement. &lt;a href='https://wiki.lesswrong.com/wiki/Instrumental_convergence' target='_blank' rel='noopener noreferrer'&gt;LessWrong Wiki&lt;/a&gt;">Instrumental Convergence</strong>: Intelligent agents, regardless of final goals, will likely pursue similar instrumental goals useful for almost any objective (e.g., self-preservation, resource acquisition, cognitive enhancement, goal integrity).</p>
<button aria-controls="collapse-instrumental-convergence" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-instrumental-convergence" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-instrumental-convergence">
<p>Unconstrained pursuit of these logical sub-goals could lead to conflict with human interests (e.g., resource competition, resisting shutdown).</p>
</div>
</div>
</div>
<div class="col-lg-4 col-md-6">
<div class="info-card" id="card-friendly-ai">
<div class="card-header-content">
<h5><i class="bi bi-person-heart"></i> Friendly AI (FAI) / Aligned AI</h5>
<div class="card-content-wrapper">
<p class="summary"><strong data-bs-html="true" data-bs-toggle="tooltip" title="Hypothetical artificial general intelligence (AGI) that would have a positive rather than negative effect on humanity. It is designed to be demonstrably beneficial, with goals robustly aligned with human values, remaining safe even if superintelligent. &lt;a href='https://wiki.lesswrong.com/wiki/Friendly_AI' target='_blank' rel='noopener noreferrer'&gt;LessWrong Wiki&lt;/a&gt;">Friendly AI (FAI) or Aligned AI</strong> is the design of AI systems that are demonstrably beneficial, with goals robustly aligned with human values, remaining safe even if superintelligent.</p>
<button aria-controls="collapse-friendly-ai" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-friendly-ai" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-friendly-ai">
<p>Challenges include the <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The difficult problem of specifying complex, nuanced, and potentially fragile human values to an AI in a way that is robust and does not lead to unintended or harmful interpretations by the AI. &lt;a href='https://www.lesswrong.com/tag/value-loading-problem' target='_blank' rel='noopener noreferrer'&gt;LessWrong on Value Loading&lt;/a&gt;">Value Loading Problem</span> (specifying complex human values), ensuring goal stability, designing for scalable oversight/corrigibility, and avoiding "<strong data-bs-html="true" data-bs-toggle="tooltip" title="When an AI achieves a literally specified goal in an unintended and harmful way, often by exploiting loopholes or ambiguities in the goal's definition. For example, an AI told to 'eliminate suffering' might decide to eliminate all humans.">perverse instantiation</strong>" of goals.</p>
</div>
</div>
</div>
<div class="col-lg-4 col-md-6">
<div class="info-card" id="card-complexity-of-value">
<div class="card-header-content">
<h5><i class="bi bi-puzzle-fill"></i> Complexity of Value</h5>
<div class="card-content-wrapper">
<p class="summary">Human values are intricate, nuanced, context-dependent, and often contradictory, making them extremely hard to fully capture and encode into an AI robustly (the "<strong data-bs-html="true" data-bs-toggle="tooltip" title="The challenge of instilling human values into an AI system. This is difficult due to the complexity, subtlety, and potential inconsistency of human values. &lt;a href='https://www.lesswrong.com/tag/value-loading-problem' target='_blank' rel='noopener noreferrer'&gt;LessWrong on Value Loading&lt;/a&gt;">Value Loading Problem</strong>").</p>
<button aria-controls="collapse-complexity-of-value" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-complexity-of-value" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-complexity-of-value">
<p>Simple instructions (e.g., "make people happy") can be <span data-bs-html="true" data-bs-toggle="tooltip" title="An AI fulfilling the literal specification of its goal in a way that is deeply contrary to the human programmer's intent, often with catastrophic consequences. For example, an AI tasked with maximizing paperclip production might convert all matter, including humans, into paperclips.">perversely instantiated</span>. Our values are an evolved system, not a simple list. Capturing this "fragile" structure is a central alignment challenge explored in The Sequences.</p>
</div>
</div>
</div>
<div class="col-lg-12">
<div class="info-card" id="card-pdoom-risk">
<div class="card-header-content">
<h5><i class="bi bi-radioactive"></i> P(doom) &amp; AI Extinction Risk Probabilities</h5>
<div class="card-content-wrapper">
<p class="summary"><span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="A shorthand term used in AI safety discussions to denote the subjective probability that unaligned Artificial General Intelligence (AGI) will lead to human extinction or a similarly catastrophic global outcome. Estimates vary widely among researchers.">"P(doom)"</span> denotes a subjective probability that unaligned AGI will cause human extinction or a similar global catastrophe. Advanced AI is considered a significant <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="A risk that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic curtailment of its potential. Key thinkers include Nick Bostrom. &lt;a href='https://www.nickbostrom.com/existential/risks.html' target='_blank' rel='noopener noreferrer'&gt;Bostrom on X-Risk&lt;/a&gt;">existential risk</span> by many, including Yudkowsky.</p>
<button aria-controls="collapse-pdoom-risk" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-pdoom-risk" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-pdoom-risk">
<h6>Understanding Existential Risk (X-risk):</h6>
<p>Threatens premature extinction of Earth-originating intelligent life or drastic curtailment of its potential.</p>
<h6>Why AGI is an X-risk:</h6>
<p>Unaligned superintelligence could outcompete humanity, transform the planet incompatibly, or cause extinction as an unintended side effect of pursuing its goals.</p>
<h6>P(doom) - Subjective Probabilities:</h6>
<p>Personal estimates of catastrophic outcomes from AGI. Estimates vary; Yudkowsky's are notably high, reflecting deep concern. Discussion of p(doom) highlights the perceived severity and urgency of AI safety. The <strong data-bs-html="true" data-bs-toggle="tooltip" title="An ethical principle stating that if an action or policy has a suspected risk of causing severe harm to the public or the environment, in the absence of scientific consensus that the action or policy is harmful, the burden of proof that it is *not* harmful falls on those taking the action.">precautionary principle</strong> is often invoked.</p>
</div>
</div>
</div>
</div>
</div>
<div class="schema-container" data-section-id="section-decision-theory">
<h2 class="section-title" id="section-decision-theory-title"><i class="bi bi-signpost-split-fill"></i> Decision Theory &amp; Thought Experiments</h2>
<div class="row">
<div class="col-lg-6 col-md-12">
<div class="info-card" id="card-navigating-uncertainty">
<div class="card-header-content">
<h5><i class="bi bi-compass-fill"></i> Navigating Uncertainty in Decisions</h5>
<div class="card-content-wrapper">
<p class="summary">Yudkowsky's work, especially in <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Eliezer Yudkowsky's extensive collection of writings on rationality, cognitive science, and AI, often cited as foundational texts in the rationalist community. &lt;a href='https://www.readthesequences.com/' target='_blank' rel='noopener noreferrer'&gt;Read The Sequences&lt;/a&gt;">The Sequences</span>, delves into advanced decision theory for scenarios with high stakes, low probabilities, and profound uncertainty relevant to AGI's future.</p>
<button aria-controls="collapse-navigating-uncertainty" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-navigating-uncertainty" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-navigating-uncertainty">
<p>The aim is to establish rational action principles with incomplete information or unprecedented possibilities, critically examining <strong data-bs-html="true" data-bs-toggle="tooltip" title="A theory in decision analysis that suggests the best action is the one that maximizes the decision-maker's expected utility. Utility is a measure of satisfaction or value. &lt;a href='https://plato.stanford.edu/entries/expected-utility/' target='_blank' rel='noopener noreferrer'&gt;SEP on Expected Utility&lt;/a&gt;">expected utility theory</strong> and its limits. Yudkowsky advocates for robust reasoning methods for "<strong data-bs-html="true" data-bs-toggle="tooltip" title="Highly improbable events with severe consequences that are difficult to predict. Coined by Nassim Nicholas Taleb. &lt;a href='https://en.wikipedia.org/wiki/Black_swan_theory' target='_blank' rel='noopener noreferrer'&gt;Wikipedia&lt;/a&gt;">black swan" events</strong>.</p>
</div>
</div>
</div>
<div class="col-lg-6 col-md-12">
<div class="info-card" id="card-pascals-mugging">
<div class="card-header-content">
<h5><i class="bi bi-exclamation-triangle-fill"></i> Pascal's Mugging</h5>
<div class="card-content-wrapper">
<p class="summary">A <strong data-bs-html="true" data-bs-toggle="tooltip" title="A thought experiment illustrating a problem in expected value decision theory. A 'mugger' offers a tiny chance of an astronomically large reward in exchange for a small immediate sum, potentially leading to absurd conclusions if expected value is naively maximized. &lt;a href='https://wiki.lesswrong.com/wiki/Pascal%27s_mugging' target='_blank' rel='noopener noreferrer'&gt;LessWrong Wiki&lt;/a&gt;">Pascal's Mugging</strong> is a thought experiment highlighting paradoxes in applying expected utility theory to extremely low-probability events with astronomically high payoffs, questioning rational decision-making in such edge cases.</p>
<button aria-controls="collapse-pascals-mugging" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-pascals-mugging" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse-content" id="collapse-pascals-mugging">
<h6>The Scenario:</h6>
<p>A "mugger" claims they will provide an immense reward (e.g., utility of saving 3^^^^3 lives) for a small sum (e.g., $5). Even a tiny probability of truth could, by naive expected utility, compel compliance.</p>
<h6>Relevance:</h6>
<p>Challenges decision frameworks with vast utilities and microscopic probabilities, relevant to <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Risks that could lead to human extinction or the irreversible collapse of civilization. AI is considered a potential source of such risks. &lt;a href='https://futureoflife.org/background/existential-risk/' target='_blank' rel='noopener noreferrer'&gt;FLI on X-Risk&lt;/a&gt;">AI existential risk</span>. It forces deeper consideration of priors, probability thresholds, and decision theories robust against "Pascalian" scenarios.</p>
</div>
</div>
</div>
</div>
</div>
<div class="schema-container" data-section-id="section-the-sequences">
<h2 class="section-title" id="section-the-sequences-title"><i class="bi bi-collection-fill"></i> The Sequences on LessWrong</h2>
<div class="row">
<div class="col-lg-12">
<div class="info-card" id="card-sequences-overview">
<div class="card-header-content">
<h5><i class="bi bi-book-half"></i> "Rationality: From AI to Zombies" - An In-Depth Guide</h5>
<div class="card-content-wrapper">
<p class="summary">"<strong data-bs-html="true" data-bs-toggle="tooltip" title="A large collection of essays by Eliezer Yudkowsky, originally published on LessWrong and Overcoming Bias between 2006 and 2009. They cover topics in human rationality, cognitive science, philosophy, and artificial intelligence. &lt;a href='https://www.readthesequences.com/' target='_blank' rel='noopener noreferrer'&gt;Read The Sequences&lt;/a&gt;">The Sequences</strong>" are hundreds of essays by Eliezer Yudkowsky (2006-2009), primarily on <a href="https://www.lesswrong.com/" rel="noopener noreferrer" target="_blank">LessWrong</a> and <a href="https://www.overcomingbias.com/" rel="noopener noreferrer" target="_blank">Overcoming Bias</a>. Organized into <a data-bs-html="true" data-bs-toggle="tooltip" href="https://www.readthesequences.com/" rel="noopener noreferrer" target="_blank" title="The compiled book form of Eliezer Yudkowsky's 'The Sequences,' offering a comprehensive exploration of rationality and AI. &lt;a href='https://www.readthesequences.com/' target='_blank' rel='noopener noreferrer'&gt;Read Online&lt;/a&gt;">"Rationality: From AI to Zombies,"</a> they are foundational texts for the rationalist community and AI safety.</p>
<button aria-controls="collapse-sequences-overview" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-sequences-overview" data-bs-toggle="collapse" type="button">
                                            Explore The Sequences <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse collapse-content" id="collapse-sequences-overview">
<p>"Rationality: From AI to Zombies" is structured into six "books." Access the compiled work at <a href="https://www.readthesequences.com/" rel="noopener noreferrer" target="_blank">readthesequences.com</a> or <a href="https://intelligence.org/rationality-ai-zombies/" rel="noopener noreferrer" target="_blank">intelligence.org</a>.</p>
<h6>Book I: Map and Territory</h6>
<p><strong>Theme:</strong> Bayesian rationality, distinguishing mental models (map) from reality (territory). Focuses on <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The branch of philosophy concerned with the nature, origin, and limits of human knowledge. It explores questions like 'What is knowledge?' and 'How do we acquire it?'. [8, 9, 16, 24, 31] &lt;a href='https://plato.stanford.edu/entries/epistemology/' target='_blank' rel='noopener noreferrer'&gt;SEP on Epistemology&lt;/a&gt;">epistemology</span>.</p>
<h6>Book II: How to Actually Change Your Mind</h6>
<p><strong>Theme:</strong> Overcoming motivated reasoning and <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="Systematic errors in thinking that can affect judgments and decisions. Examples include confirmation bias and availability heuristic. &lt;a href='https://www.simplypsychology.org/cognitive-bias.html' target='_blank' rel='noopener noreferrer'&gt;Simply Psychology&lt;/a&gt;">cognitive biases</span> in belief formation.</p>
<h6>Book III: The Machine in the Ghost</h6>
<p><strong>Theme:</strong> Minds, goals, concepts, and the nature of intelligence, often paralleled with AI. Explores <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="A branch of philosophy that studies the nature of the mind, mental events, mental functions, mental properties, consciousness, and their relationship with the physical body, particularly the brain. &lt;a href='https://plato.stanford.edu/entries/philosophy-mind/' target='_blank' rel='noopener noreferrer'&gt;SEP on Phil. of Mind&lt;/a&gt;">philosophy of mind</span> and <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The objectives or desired states that an agent (human or AI) strives to achieve. Understanding and specifying goal systems is crucial for AI alignment.">goal systems</span>.</p>
<h6>Book IV: Mere Reality</h6>
<p><strong>Theme:</strong> Science, the physical world, and their relation to rational inference. Tackles <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The study of the nature and justification of scientific knowledge, methods, and theories. It examines how science produces knowledge and the reliability of scientific claims.">scientific epistemology</span> and <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The branch of metaphysics dealing with the nature of being, existence, or reality. It asks what entities exist or can be said to exist, and how such entities can be grouped. [12, 17, 19, 28, 36] &lt;a href='https://plato.stanford.edu/entries/ontology-computer-science/' target='_blank' rel='noopener noreferrer'&gt;SEP on Ontology (CompSci)&lt;/a&gt;">ontology</span>.</p>
<h6>Book V: Mere Goodness</h6>
<p><strong>Theme:</strong> Human values, <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The branch of ethics that studies the nature of ethical properties, statements, attitudes, and judgments. It explores questions like 'What is goodness?' and 'Are moral truths objective?'. &lt;a href='https://plato.stanford.edu/entries/metaethics/' target='_blank' rel='noopener noreferrer'&gt;SEP on Metaethics&lt;/a&gt;">meta-ethics</span>, and defining "goodness." Crucial for <span class="term" data-bs-html="true" data-bs-toggle="tooltip" title="The research area focused on ensuring that artificial intelligence systems' goals and behaviors are aligned with human values and ethical principles. [2, 3, 7, 27] &lt;a href='https://www.weforum.org/agenda/2023/10/ai-value-alignment-human-values/' target='_blank' rel='noopener noreferrer'&gt;WEF on Value Alignment&lt;/a&gt; [27]">AI value alignment</span>.</p>
<h6>Book VI: Becoming Stronger</h6>
<p><strong>Theme:</strong> Self-improvement, group rationality, and practical applications. Focuses on applied rationality.</p>
<p>These sequences collectively aim to provide a toolkit for improving reasoning and decision-making, with significant implications for AI challenges.</p>
</div>
</div>
</div>
</div>
</div>
<div class="schema-container" data-section-id="section-legacy-further-exploration">
<h2 class="section-title" id="section-legacy-further-exploration-title"><i class="bi bi-search-heart"></i> Additional Resources</h2>
<div class="row">
<div class="col-lg-12">
<div class="info-card" id="card-legacy-key-resources">
<div class="card-header-content">
<h5><i class="bi bi-building"></i> Key Organizations &amp; General Reading</h5>
<div class="card-content-wrapper">
<p class="summary">For ongoing research, refer to the <strong data-bs-html="true" data-bs-toggle="tooltip" title="Machine Intelligence Research Institute: A non-profit organization co-founded by Eliezer Yudkowsky, focused on foundational mathematical research to ensure future AI systems are safe and beneficial. &lt;a href='https://intelligence.org' target='_blank' rel='noopener noreferrer'&gt;Visit MIRI&lt;/a&gt;">Machine Intelligence Research Institute (MIRI)</strong>. For broader context, consider works on decision theory, AI ethics, and cognitive psychology.</p>
<button aria-controls="collapse-legacy-key-resources" aria-expanded="false" class="details-toggle" data-bs-target="#collapse-legacy-key-resources" data-bs-toggle="collapse" type="button">
                                            Details <span class="toggle-icon">â–¼</span>
</button>
</div>
</div>
<div class="collapse collapse-content" id="collapse-legacy-key-resources">
<ul>
<li><strong><a href="https://intelligence.org" rel="noopener noreferrer" target="_blank">Machine Intelligence Research Institute (MIRI)</a>:</strong> Co-founded by Yudkowsky, MIRI conducts formal research on AI alignment.</li>
<li><strong>General Reading Suggestions:</strong>
<ul>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="Swedish philosopher and Director of the Future of Humanity Institute at Oxford University, known for his work on existential risk, the simulation argument, and AI. &lt;a href='https://nickbostrom.com/' target='_blank' rel='noopener noreferrer'&gt;Nick Bostrom's Website&lt;/a&gt;">Nick Bostrom</strong>: "<strong data-bs-html="true" data-bs-toggle="tooltip" title="A book by Nick Bostrom that analyzes the potential long-term consequences of developing artificial superintelligence, outlining paths, dangers, and strategies. &lt;a href='https://www.nickbostrom.com/superintelligence.html' target='_blank' rel='noopener noreferrer'&gt;Book Information&lt;/a&gt;">Superintelligence: Paths, Dangers, Strategies</strong>."</li>
<li>Works on <strong data-bs-html="true" data-bs-toggle="tooltip" title="The study of strategic interaction between rational decision-makers. It has applications in economics, political science, psychology, and AI. &lt;a href='https://plato.stanford.edu/entries/game-theory/' target='_blank' rel='noopener noreferrer'&gt;SEP on Game Theory&lt;/a&gt;">Game Theory</strong> and <strong data-bs-html="true" data-bs-toggle="tooltip" title="The study of how decisions are made, or should be made, under conditions of uncertainty or conflict. It involves analyzing choices to identify the optimal decision. &lt;a href='https://plato.stanford.edu/entries/decision-theory/' target='_blank' rel='noopener noreferrer'&gt;SEP on Decision Theory&lt;/a&gt;">Decision Theory</strong>.</li>
<li><strong data-bs-html="true" data-bs-toggle="tooltip" title="Israeli-American psychologist and economist, Nobel laureate known for his work on the psychology of judgment and decision-making, as well as behavioral economics. &lt;a href='https://kahneman.socialpsychology.org/' target='_blank' rel='noopener noreferrer'&gt;More Info&lt;/a&gt;">Daniel Kahneman</strong>: "<strong data-bs-html="true" data-bs-toggle="tooltip" title="A best-selling book by Daniel Kahneman that summarizes his research on cognitive biases, heuristics, and the two systems of thought (fast, intuitive System 1, and slow, deliberate System 2). &lt;a href='https://us.macmillan.com/books/9780374533557/thinkingfastandslow' target='_blank' rel='noopener noreferrer'&gt;Book Information&lt;/a&gt;">Thinking, Fast and Slow</strong>."</li>
<li>Literature on Ethics of Artificial Intelligence.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</main>
<footer class="page-footer">
<p class="mb-1" style="font-size: 0.9em; color: var(--text-color-muted);">
        This cheatsheet draws inspiration from the extensive works of Eliezer Yudkowsky, the research and publications by the Machine Intelligence Research Institute (MIRI), discussions on LessWrong, and the broader AI Safety and Rationality communities.
    </p>
<p class="mb-2">
        Cheatsheet by David Veksler Â© <span id="currentYearFooter">2025</span>. All rights reserved.
    </p>
<div class="mb-3">
<h6 style="color: var(--text-color-primary); font-family: var(--font-headings); font-size: 1rem; margin-bottom: 0.5rem;">Explore Related AI Cheatsheets:</h6>
<a class="mx-2" href="https://cheatsheets.davidveksler.com/airisk.html" rel="noopener noreferrer" style="font-size: 0.9em;" target="_blank" title="AI Existential Risk (X-Risk) Cheatsheet">
<i class="bi bi-shield-exclamation"></i> AI X-Risk
        </a>
<a class="mx-2" href="https://cheatsheets.davidveksler.com/ai-frontier.html" rel="noopener noreferrer" style="font-size: 0.9em;" target="_blank" title="AI Frontier Model Builders Cheatsheet">
<i class="bi bi-cpu-fill"></i> AI Frontier Models
        </a>
<a class="mx-2" href="https://cheatsheets.davidveksler.com/aisafety.html" rel="noopener noreferrer" style="font-size: 0.9em;" target="_blank" title="Interactive AI Safety Ecosystem Hub">
<i class="bi bi-diagram-3-fill"></i> AI Safety Ecosystem
        </a>
</div>
<div>
<a class="mx-2" href="https://www.linkedin.com/in/davidveksler/" rel="noopener noreferrer" target="_blank" title="David Veksler on LinkedIn">
<i class="bi bi-linkedin"></i> LinkedIn
        </a>
<a class="mx-2" href="https://cheatsheets.davidveksler.com/" rel="noopener noreferrer" target="_blank" title="Browse All Cheatsheets by David Veksler">
<i class="bi bi-collection-fill"></i> All Cheatsheets
        </a>
</div>
<script>
        // This script should be part of the main page's script logic
        // or placed just before the closing </body> tag.
        // It updates the copyright year in the footer.
        const currentYearFooterSpan = document.getElementById('currentYearFooter');
        if (currentYearFooterSpan) {
            currentYearFooterSpan.textContent = new Date().getFullYear();
        }
    </script>
</footer>
<!-- Bootstrap JS Bundle -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
<script>
        document.addEventListener('DOMContentLoaded', function () {
            const currentYearSpan = document.getElementById('currentYear');
            if (currentYearSpan) {
                currentYearSpan.textContent = new Date().getFullYear();
            }

            // Tooltip Initialization (inspired by airisk.html)
            // Extend Bootstrap's default allowList for tooltips to ensure links with target and rel work
            const defaultAllowList = bootstrap.Tooltip.Default.allowList;
            defaultAllowList.a.push('target', 'rel', 'href'); // Ensure target, rel, and href are allowed on <a> tags
            defaultAllowList.strong = []; // Allow strong tags
            defaultAllowList.p = []; // Allow p tags
            defaultAllowList.ul = []; // Allow ul tags
            defaultAllowList.li = []; // Allow li tags
            defaultAllowList.br = []; // Allow br tags


            var tooltipTriggerList = Array.from(document.querySelectorAll('[data-bs-toggle="tooltip"]'));
            var tooltipList = tooltipTriggerList.map(function (tooltipTriggerEl) {
              return new bootstrap.Tooltip(tooltipTriggerEl, {
                  html: true,
                  trigger: 'hover focus', // Show on hover and focus for accessibility
                  delay: { "show": 150, "hide": 700 }, // Quick show, slightly slower hide
                  sanitize: true,
                  allowList: defaultAllowList, // Apply the extended allowList
                  boundary: 'scrollParent' // Helps keep tooltips from being clipped
               });
            });


            // Collapse functionality (existing script)
            const mainContainer = document.querySelector('.container-main');
            if (mainContainer) {
                mainContainer.addEventListener('click', function(event) {
                    const toggleButton = event.target.closest('.details-toggle');
                    if (toggleButton) {
                        const targetId = toggleButton.getAttribute('data-bs-target');
                        if (targetId) {
                            const content = document.querySelector(targetId);
                            // Ensure we don't interfere with Bootstrap's own collapse instance if it exists
                            const bsCollapseInstance = bootstrap.Collapse.getInstance(content);
                            if (!bsCollapseInstance) { // If no BS instance, manage manually (less ideal but fallback)
                                if (content) {
                                    content.classList.toggle('active');
                                    if (content.style.maxHeight && content.style.maxHeight !== "0px") {
                                        content.style.maxHeight = null;
                                    } else {
                                        content.style.maxHeight = content.scrollHeight + "px";
                                    }
                                }
                            }
                        }
                    }
                });

                // Sync aria-expanded with Bootstrap's events for our custom toggles
                document.querySelectorAll('.collapse-content').forEach(content => {
                    const toggleButton = document.querySelector(`.details-toggle[data-bs-target="#${content.id}"]`);
                    if (toggleButton) {
                        // Listener for when Bootstrap starts to show the collapse element
                        content.addEventListener('show.bs.collapse', function () {
                            toggleButton.setAttribute('aria-expanded', 'true');
                            if (!content.classList.contains('active')) {
                                content.classList.add('active');
                                // Max-height will be handled by Bootstrap, but if our transition relies on it:
                                content.style.maxHeight = content.scrollHeight + "px";
                            }
                        });
                        // Listener for when Bootstrap has finished showing the collapse element
                        content.addEventListener('shown.bs.collapse', function() {
                           // Recalculate maxHeight in case content inside changed during animation or for stability
                           content.style.maxHeight = content.scrollHeight + "px";
                        });
                        // Listener for when Bootstrap starts to hide the collapse element
                        content.addEventListener('hide.bs.collapse', function () {
                            toggleButton.setAttribute('aria-expanded', 'false');
                            // Max-height will be handled by Bootstrap, but if our transition relies on it:
                            content.style.maxHeight = null;
                            // Remove 'active' class as Bootstrap finishes hiding
                        });
                        // Listener for when Bootstrap has finished hiding the collapse element
                         content.addEventListener('hidden.bs.collapse', function() {
                            content.style.maxHeight = null; // Ensure it's fully collapsed
                            if (content.classList.contains('active')) {
                                content.classList.remove('active');
                            }
                        });
                    }
                });
            }
        });
    </script>
</body>
</html>